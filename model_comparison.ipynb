{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28dac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úì All libraries imported successfully!\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d5646",
   "metadata": {},
   "source": [
    "## 1. Load Best Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from each model's directory\n",
    "results_dirs = {\n",
    "    'SimpleNN': 'simplenn_results',\n",
    "    'MLP': 'mlp_results',\n",
    "    'CNN': 'cnn_results',\n",
    "    'Transfer Learning': 'transfer_learning_results'\n",
    "}\n",
    "\n",
    "# Function to load the most recent summary report\n",
    "def load_latest_summary(results_dir):\n",
    "    \"\"\"Load the most recent summary report from results directory\"\"\"\n",
    "    try:\n",
    "        # Find all summary_report files\n",
    "        files = [f for f in os.listdir(results_dir) if f.startswith('summary_report_')]\n",
    "        if not files:\n",
    "            print(f\"  ‚úó No summary reports found in {results_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Sort by timestamp (most recent first)\n",
    "        latest_file = sorted(files, reverse=True)[0]\n",
    "        \n",
    "        with open(os.path.join(results_dir, latest_file), 'r') as f:\n",
    "            summary = json.load(f)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error loading summary from {results_dir}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load all summaries\n",
    "print(\"Loading model results...\\n\")\n",
    "all_summaries = {}\n",
    "for model_name, results_dir in results_dirs.items():\n",
    "    print(f\"  Loading {model_name}...\")\n",
    "    summary = load_latest_summary(results_dir)\n",
    "    if summary:\n",
    "        all_summaries[model_name] = summary\n",
    "        print(f\"    ‚úì Val Acc: {summary['best_val_accuracy']:.4f}, Test Acc: {summary['best_test_accuracy']:.4f}\")\n",
    "    else:\n",
    "        print(f\"    ‚úó Failed to load\")\n",
    "\n",
    "print(f\"\\n‚úì Successfully loaded {len(all_summaries)}/4 models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887f931",
   "metadata": {},
   "source": [
    "## 2. Performance Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf57ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, summary in all_summaries.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Val Accuracy': summary['best_val_accuracy'],\n",
    "        'Test Accuracy': summary['best_test_accuracy'],\n",
    "        'Tuning Combinations': summary['total_combinations'],\n",
    "        'Top 5 Mean Val Acc': np.mean(summary['top5_accuracies']),\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTER-MODEL COMPARISON\".center(80))\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPerformance Metrics (sorted by Test Accuracy):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed3b96c",
   "metadata": {},
   "source": [
    "## 3. Best Hyperparameters Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ed656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best hyperparameters for each model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST HYPERPARAMETERS BY MODEL\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, summary in all_summaries.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Best Validation Accuracy: {summary['best_val_accuracy']:.4f}\")\n",
    "    print(f\"  Best Test Accuracy: {summary['best_test_accuracy']:.4f}\")\n",
    "    print(f\"  \\n  Hyperparameters:\")\n",
    "    for key, value in summary['best_hyperparameters'].items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1ac39",
   "metadata": {},
   "source": [
    "## 4. Accuracy Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f878b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Inter-Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Validation Accuracy Comparison\n",
    "ax = axes[0, 0]\n",
    "models = list(all_summaries.keys())\n",
    "val_accs = [all_summaries[m]['best_val_accuracy'] for m in models]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "bars1 = ax.bar(models, val_accs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Validation Accuracy', fontweight='bold')\n",
    "ax.set_title('Validation Accuracy Comparison', fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, acc in zip(bars1, val_accs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 2. Test Accuracy Comparison\n",
    "ax = axes[0, 1]\n",
    "test_accs = [all_summaries[m]['best_test_accuracy'] for m in models]\n",
    "bars2 = ax.bar(models, test_accs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Test Accuracy', fontweight='bold')\n",
    "ax.set_title('Test Accuracy Comparison', fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, acc in zip(bars2, test_accs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 3. Top 5 Average Validation Accuracy\n",
    "ax = axes[1, 0]\n",
    "top5_avgs = [np.mean(all_summaries[m]['top5_accuracies']) for m in models]\n",
    "bars3 = ax.bar(models, top5_avgs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Average Validation Accuracy', fontweight='bold')\n",
    "ax.set_title('Top 5 Average Validation Accuracy', fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, avg in zip(bars3, top5_avgs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{avg:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 4. Accuracy Gap (Val - Test)\n",
    "ax = axes[1, 1]\n",
    "gaps = [all_summaries[m]['best_val_accuracy'] - all_summaries[m]['best_test_accuracy'] for m in models]\n",
    "bars4 = ax.bar(models, gaps, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Accuracy Gap (Val - Test)', fontweight='bold')\n",
    "ax.set_title('Overfitting Analysis (Gap)', fontweight='bold')\n",
    "ax.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, gap in zip(bars4, gaps):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{gap:.4f}', ha='center', va='bottom' if gap >= 0 else 'top', fontweight='bold')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('inter_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Comparison plot saved as 'inter_model_comparison.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a5216",
   "metadata": {},
   "source": [
    "## 5. Loss Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79281ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load training history\n",
    "def load_best_history(results_dir):\n",
    "    \"\"\"Load the best model's training history\"\"\"\n",
    "    try:\n",
    "        files = [f for f in os.listdir(results_dir) if f.startswith('best_model_history_')]\n",
    "        if not files:\n",
    "            return None\n",
    "        latest_file = sorted(files, reverse=True)[0]\n",
    "        with open(os.path.join(results_dir, latest_file), 'rb') as f:\n",
    "            history = pickle.load(f)\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading history from {results_dir}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load histories for all models\n",
    "print(\"\\nLoading training histories...\")\n",
    "histories = {}\n",
    "for model_name, results_dir in results_dirs.items():\n",
    "    history = load_best_history(results_dir)\n",
    "    if history:\n",
    "        histories[model_name] = history\n",
    "        print(f\"  ‚úì {model_name}: {len(history['train_loss'])} epochs\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {model_name}: Failed to load\")\n",
    "\n",
    "print(f\"\\n‚úì Successfully loaded {len(histories)}/4 histories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss curves for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Training Loss Curves - All Models (Best Configuration)', fontsize=16, fontweight='bold')\n",
    "\n",
    "model_names = list(histories.keys())\n",
    "colors_loss = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "# Individual plots for each model\n",
    "for idx, (model_name, history) in enumerate(histories.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2.5, marker='o', markersize=5)\n",
    "    ax.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2.5, marker='s', markersize=5)\n",
    "    \n",
    "    ax.set_title(f'{model_name}', fontweight='bold', fontsize=13)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='upper right', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_curves_all_models.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Loss curves plot saved as 'loss_curves_all_models.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overlaid loss curves for comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Normalize epochs to 0-1 for comparison (since models train for different durations)\n",
    "colors_map = {'SimpleNN': 'blue', 'MLP': 'green', 'CNN': 'red', 'Transfer Learning': 'purple'}\n",
    "\n",
    "# Training Loss\n",
    "for model_name, history in histories.items():\n",
    "    epochs_normalized = np.linspace(0, 1, len(history['train_loss']))\n",
    "    ax1.plot(epochs_normalized, history['train_loss'], label=model_name, \n",
    "             color=colors_map.get(model_name, 'black'), linewidth=2.5, marker='o', markersize=4)\n",
    "\n",
    "ax1.set_title('Training Loss Comparison (Normalized Epochs)', fontweight='bold', fontsize=13)\n",
    "ax1.set_xlabel('Normalized Epoch')\n",
    "ax1.set_ylabel('Training Loss')\n",
    "ax1.legend(fontsize=11, loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "for model_name, history in histories.items():\n",
    "    epochs_normalized = np.linspace(0, 1, len(history['val_loss']))\n",
    "    ax2.plot(epochs_normalized, history['val_loss'], label=model_name, \n",
    "             color=colors_map.get(model_name, 'black'), linewidth=2.5, marker='s', markersize=4)\n",
    "\n",
    "ax2.set_title('Validation Loss Comparison (Normalized Epochs)', fontweight='bold', fontsize=13)\n",
    "ax2.set_xlabel('Normalized Epoch')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.legend(fontsize=11, loc='best')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_curves_comparison_normalized.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Normalized loss curves plot saved as 'loss_curves_comparison_normalized.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417cbb3",
   "metadata": {},
   "source": [
    "## 6. Final Ranking and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d455cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final ranking\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RANKING - BEST TO WORST\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "ranking = comparison_df.sort_values('Test Accuracy', ascending=False).reset_index(drop=True)\n",
    "ranking['Rank'] = range(1, len(ranking) + 1)\n",
    "\n",
    "print(\"\\n\" + ranking[['Rank', 'Model', 'Val Accuracy', 'Test Accuracy', 'Top 5 Mean Val Acc']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = ranking.iloc[0]\n",
    "print(f\"\\nüèÜ BEST OVERALL MODEL: {best_model['Model']}\")\n",
    "print(f\"   - Test Accuracy: {best_model['Test Accuracy']:.4f}\")\n",
    "print(f\"   - Validation Accuracy: {best_model['Val Accuracy']:.4f}\")\n",
    "print(f\"   - Recommendation: Use this model for production deployment\")\n",
    "\n",
    "# Check for overfitting\n",
    "gaps = comparison_df.copy()\n",
    "gaps['Gap'] = gaps['Val Accuracy'] - gaps['Test Accuracy']\n",
    "best_generalization = gaps.sort_values('Gap').iloc[0]\n",
    "\n",
    "print(f\"\\nüí™ BEST GENERALIZATION (Lowest Overfitting): {best_generalization['Model']}\")\n",
    "print(f\"   - Val-Test Gap: {best_generalization['Gap']:.4f}\")\n",
    "print(f\"   - Recommendation: More robust to unseen data\")\n",
    "\n",
    "# Most efficient\n",
    "efficient = ranking.sort_values('Tuning Combinations').iloc[0]\n",
    "print(f\"\\n‚ö° MOST EFFICIENT: {efficient['Model']}\")\n",
    "print(f\"   - Tuning Combinations: {int(efficient['Tuning Combinations'])}\")\n",
    "print(f\"   - Test Accuracy: {efficient['Test Accuracy']:.4f}\")\n",
    "print(f\"   - Recommendation: Faster tuning process\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0612b",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive comparison report\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "report = {\n",
    "    'timestamp': timestamp,\n",
    "    'comparison': comparison_df.to_dict('records'),\n",
    "    'best_model': {\n",
    "        'name': best_model['Model'],\n",
    "        'test_accuracy': float(best_model['Test Accuracy']),\n",
    "        'val_accuracy': float(best_model['Val Accuracy']),\n",
    "        'hyperparameters': all_summaries[best_model['Model']]['best_hyperparameters']\n",
    "    },\n",
    "    'all_model_details': {model: summary for model, summary in all_summaries.items()}\n",
    "}\n",
    "\n",
    "with open(f'inter_model_comparison_report_{timestamp}.json', 'w') as f:\n",
    "    json.dump(report, f, indent=4, default=str)\n",
    "\n",
    "print(f\"\\n‚úì Comprehensive comparison report saved to: inter_model_comparison_report_{timestamp}.json\")\n",
    "\n",
    "# Save comparison table as CSV\n",
    "comparison_df.to_csv(f'inter_model_comparison_{timestamp}.csv', index=False)\n",
    "print(f\"‚úì Comparison table saved to: inter_model_comparison_{timestamp}.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"INTER-MODEL COMPARISON COMPLETE!\".center(80))\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
