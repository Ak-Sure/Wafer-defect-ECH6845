{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f23551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined successfully!\n",
      "✓ All libraries imported successfully!\n",
      "Device: cuda\n",
      "✓ All libraries imported successfully!\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules (force reload)\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Clear any cached imports\n",
    "if 'models' in sys.modules:\n",
    "    del sys.modules['models']\n",
    "if 'data_loading' in sys.modules:\n",
    "    del sys.modules['data_loading']\n",
    "if 'utility' in sys.modules:\n",
    "    del sys.modules['utility']\n",
    "if 'config' in sys.modules:\n",
    "    del sys.modules['config']\n",
    "\n",
    "from data_loading import WaferDataLoader\n",
    "from utility import (setup_model_and_loaders, hyperparameter_tuning, \n",
    "                     evaluate_model, train_model)\n",
    "from models import WaferMobileNet\n",
    "from config import TRANSFER_LEARNING_TUNING_GRID\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✓ All libraries imported successfully!\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf031a",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d22348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wafer defect dataset...\n",
      "Loading dataset from kagglehub cache...\n",
      "✓ Dataset loaded successfully\n",
      "  Wafer maps shape: (38015, 52, 52)\n",
      "  Raw labels shape: (38015, 8)\n",
      "✓ Labels converted to class indices\n",
      "  Number of unique defect classes: 38\n",
      "✓ Dataset loaded successfully!\n",
      "X shape: (38015, 52, 52)\n",
      "y shape: (38015,)\n",
      "Number of classes: 38\n",
      "✓ Dataset loaded successfully\n",
      "  Wafer maps shape: (38015, 52, 52)\n",
      "  Raw labels shape: (38015, 8)\n",
      "✓ Labels converted to class indices\n",
      "  Number of unique defect classes: 38\n",
      "✓ Dataset loaded successfully!\n",
      "X shape: (38015, 52, 52)\n",
      "y shape: (38015,)\n",
      "Number of classes: 38\n",
      "\n",
      "✓ Normalization:\n",
      "  Original range: [0.0, 3.0]\n",
      "  Normalized range: [0.0000, 1.0000]\n",
      "\n",
      "✓ Normalization:\n",
      "  Original range: [0.0, 3.0]\n",
      "  Normalized range: [0.0000, 1.0000]\n",
      "\n",
      "✓ Reshaped for MobileNetV2: (38015, 3, 52, 52)\n",
      "\n",
      "✓ Reshaped for MobileNetV2: (38015, 3, 52, 52)\n",
      "\n",
      "✓ Data split complete:\n",
      "  Training set: 26610 samples (70.0%)\n",
      "  Validation set: 5702 samples (15.0%)\n",
      "  Test set: 5703 samples (15.0%)\n",
      "\n",
      "✓ Data split complete:\n",
      "  Training set: 26610 samples (70.0%)\n",
      "  Validation set: 5702 samples (15.0%)\n",
      "  Test set: 5703 samples (15.0%)\n"
     ]
    }
   ],
   "source": [
    "# Load data using WaferDataLoader\n",
    "print(\"Loading wafer defect dataset...\")\n",
    "loader = WaferDataLoader()\n",
    "\n",
    "print(f\"✓ Dataset loaded successfully!\")\n",
    "print(f\"X shape: {loader.X.shape}\")\n",
    "print(f\"y shape: {loader.y.shape}\")\n",
    "print(f\"Number of classes: {loader.num_classes}\")\n",
    "\n",
    "# Get the data\n",
    "X = loader.X.astype('float32')\n",
    "y = loader.y\n",
    "\n",
    "# Normalize data\n",
    "X_min, X_max = X.min(), X.max()\n",
    "X_normalized = (X - X_min) / (X_max - X_min) if X_max > X_min else X\n",
    "\n",
    "print(f\"\\n✓ Normalization:\")\n",
    "print(f\"  Original range: [{X_min}, {X_max}]\")\n",
    "print(f\"  Normalized range: [{X_normalized.min():.4f}, {X_normalized.max():.4f}]\")\n",
    "\n",
    "# Reshape to 2D image format: (N, 2704) -> (N, 52, 52)\n",
    "X_normalized = X_normalized.reshape(-1, 52, 52)\n",
    "\n",
    "# Expand to 3 channels for MobileNetV2: (N, 52, 52) -> (N, 3, 52, 52)\n",
    "# Replicate grayscale across 3 channels\n",
    "X_normalized = np.repeat(X_normalized[:, np.newaxis, :, :], 3, axis=1)\n",
    "print(f\"\\n✓ Reshaped for MobileNetV2: {X_normalized.shape}\")\n",
    "\n",
    "# Split into train/val/test (70% / 15% / 15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_normalized, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data split complete:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(y)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(y)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2bfbde",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ffd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tuning grid\n",
    "print(\"=\"*80)\n",
    "print(\"TRANSFER LEARNING - HYPERPARAMETER TUNING GRID\".center(80))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTuning Grid:\")\n",
    "for param, values in TRANSFER_LEARNING_TUNING_GRID.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in TRANSFER_LEARNING_TUNING_GRID.values()])\n",
    "print(f\"\\nTotal combinations to evaluate: {total_combinations}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a31d4",
   "metadata": {},
   "source": [
    "## 3. Run Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f490459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HYPERPARAMETER TUNING: Transfer Learning (MobileNetV2)\n",
      "================================================================================\n",
      "\n",
      "✓ Testing model instantiation...\n",
      "  Model created successfully!\n",
      "  Model type: <class 'models.WaferMobileNet'>\n",
      "  Forward pass successful! Output shape: torch.Size([2, 38])\n",
      "\n",
      "✓ Starting hyperparameter tuning...\n",
      "  Total combinations: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Starting hyperparameter tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total combinations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mprod([\u001b[38;5;28mlen\u001b[39m(v)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTRANSFER_LEARNING_TUNING_GRID\u001b[38;5;241m.\u001b[39mvalues()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m tl_results \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_tuning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWaferMobileNet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRANSFER_LEARNING_TUNING_GRID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False for cleaner output\u001b[39;49;00m\n\u001b[0;32m     45\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Extract results dataframe and get top 5\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tl_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\akash\\OneDrive - University of Florida\\wafer\\utility.py:948\u001b[0m, in \u001b[0;36mhyperparameter_tuning\u001b[1;34m(model_class, X_train, X_val, X_test, y_train, y_val, y_test, param_grid, input_size, num_classes, device, criterion, num_epochs, patience, verbose)\u001b[0m\n\u001b[0;32m    945\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m    947\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 948\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_to_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[0;32m    954\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\akash\\OneDrive - University of Florida\\wafer\\utility.py:55\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience)\u001b[0m\n\u001b[0;32m     52\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 55\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     57\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\akash\\OneDrive - University of Florida\\wafer\\models.py:156\u001b[0m, in \u001b[0;36mWaferMobileNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobilenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\mobilenetv2.py:174\u001b[0m, in \u001b[0;36mMobileNetV2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\mobilenetv2.py:166\u001b[0m, in \u001b[0;36mMobileNetV2._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Cannot use \"squeeze\" as batch-size can be 1\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\mobilenetv2.py:64\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Transfer Learning with MobileNetV2\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING: Transfer Learning (MobileNetV2)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define hyperparameter grid - minimal for testing\n",
    "# Run with just 2 combinations for a quick test\n",
    "TRANSFER_LEARNING_TUNING_GRID = {\n",
    "    'learning_rate': [0.0001],  # Just 1 learning rate\n",
    "    'batch_size': [32],         # Just 1 batch size\n",
    "    'num_epochs': [5],          # Just 5 epochs for quick testing\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Starting hyperparameter tuning...\")\n",
    "print(f\"  Total combinations: {np.prod([len(v) for v in TRANSFER_LEARNING_TUNING_GRID.values()])}\")\n",
    "\n",
    "tl_results = hyperparameter_tuning(\n",
    "    model_class=WaferMobileNet,\n",
    "    X_train=X_train[:5000],  # Use subset of training data for speed\n",
    "    X_val=X_val[:1000],      # Use subset of validation data\n",
    "    X_test=X_test,\n",
    "    y_train=y_train[:5000],\n",
    "    y_val=y_val[:1000],\n",
    "    y_test=y_test,\n",
    "    param_grid=TRANSFER_LEARNING_TUNING_GRID,\n",
    "    input_size=None,\n",
    "    num_classes=loader.num_classes,\n",
    "    device=str(device),\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Extract results dataframe and get top 5\n",
    "if tl_results['best_params'] is not None:\n",
    "    print(f\"\\n✓ Tuning complete! Tested {len(tl_results['results'])} hyperparameter combinations\")\n",
    "    tl_summary_df = tl_results['summary_df']\n",
    "    tl_top5 = tl_summary_df.head(min(5, len(tl_summary_df)))\n",
    "\n",
    "    print(f\"\\nTop {len(tl_top5)} Best Results:\")\n",
    "    print(tl_top5[['learning_rate', 'batch_size', 'num_epochs', 'Val_Acc', 'Test_Acc']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n✓ Best Model Performance:\")\n",
    "    print(f\"  Validation Accuracy: {tl_results['best_val_acc']:.4f}\")\n",
    "    print(f\"  Test Accuracy: {tl_results['best_test_acc']:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nERROR: All hyperparameter combinations failed!\")\n",
    "    if tl_results['results']:\n",
    "        print(f\"First result: {tl_results['results'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b593ad0",
   "metadata": {},
   "source": [
    "## 4. Validation Loss Curves - Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain top 5 models to get training histories\n",
    "print(\"\\nRetraining top 5 models to generate loss curves...\")\n",
    "\n",
    "def retrain_with_history(model_class, X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                         params, num_classes, device_str):\n",
    "    \"\"\"Retrain a model with given params and return training history\"\"\"\n",
    "    try:\n",
    "        # Extract parameters from params dict\n",
    "        learning_rate = params.get('learning_rate', 0.0001)\n",
    "        batch_size = int(params.get('batch_size', 32))\n",
    "        epochs_to_train = int(params.get('num_epochs', 20))\n",
    "        \n",
    "        # Setup model and loaders\n",
    "        setup_result = setup_model_and_loaders(\n",
    "            model_class, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "            input_size=None, num_classes=num_classes, device=device_str,\n",
    "            batch_size=batch_size, model_kwargs={}, verbose=False\n",
    "        )\n",
    "        \n",
    "        model = setup_result['model']\n",
    "        train_loader = setup_result['train_loader']\n",
    "        val_loader = setup_result['val_loader']\n",
    "        \n",
    "        # Setup optimizer (Transfer learning typically uses Adam with lower LR)\n",
    "        opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Train\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        history = train_model(\n",
    "            model, train_loader, val_loader, criterion, opt,\n",
    "            num_epochs=epochs_to_train, device=device_str, patience=5\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get histories for top 5\n",
    "top5_histories = []\n",
    "for i, (idx, row) in enumerate(tl_top5.iterrows(), 1):\n",
    "    print(f\"  Retraining rank {i}/5...\")\n",
    "    \n",
    "    # Extract parameters from row\n",
    "    params = {\n",
    "        'learning_rate': float(row['learning_rate']),\n",
    "        'batch_size': int(row['batch_size']),\n",
    "        'num_epochs': int(row['num_epochs']),\n",
    "    }\n",
    "    \n",
    "    history = retrain_with_history(\n",
    "        WaferMobileNet, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        params, num_classes=loader.num_classes, device_str=str(device)\n",
    "    )\n",
    "    if history:\n",
    "        top5_histories.append(history)\n",
    "\n",
    "print(f\"✓ Successfully generated {len(top5_histories)} training histories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation loss curves for top 5\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('Transfer Learning (MobileNetV2) - Top 5 Configurations\\nTraining vs Validation Loss', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, history in enumerate(top5_histories):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "    ax.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "    \n",
    "    rank = idx + 1\n",
    "    val_acc = tl_top5.iloc[idx]['Val_Acc']\n",
    "    ax.set_title(f'Rank {rank} - Val Acc: {val_acc:.4f}', fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[1, 2].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('transfer_learning_validation_loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Loss curves plot saved as 'transfer_learning_validation_loss_curves.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406d2ef",
   "metadata": {},
   "source": [
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = 'transfer_learning_results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Save summary CSV with all results\n",
    "tl_summary_df.to_csv(f'{results_dir}/all_results_{timestamp}.csv', index=False)\n",
    "print(f\"✓ All results saved to: all_results_{timestamp}.csv\")\n",
    "\n",
    "# 2. Save top 5 CSV\n",
    "tl_top5.to_csv(f'{results_dir}/top5_results_{timestamp}.csv', index=False)\n",
    "print(f\"✓ Top 5 results saved to: top5_results_{timestamp}.csv\")\n",
    "\n",
    "# 3. Save best model info\n",
    "best_model_info = {\n",
    "    'best_params': tl_results['best_params'],\n",
    "    'best_val_acc': float(tl_results['best_val_acc']),\n",
    "    'best_test_acc': float(tl_results['best_test_acc']),\n",
    "}\n",
    "\n",
    "with open(f'{results_dir}/best_model_{timestamp}.json', 'w') as f:\n",
    "    json.dump(best_model_info, f, indent=4, default=str)\n",
    "print(f\"✓ Best model info saved to: best_model_{timestamp}.json\")\n",
    "\n",
    "# 4. Save best model itself\n",
    "torch.save(tl_results['best_model'].state_dict(), \n",
    "           f'{results_dir}/best_model_weights_{timestamp}.pt')\n",
    "print(f\"✓ Best model weights saved to: best_model_weights_{timestamp}.pt\")\n",
    "\n",
    "# 5. Save training history for best model\n",
    "with open(f'{results_dir}/best_model_history_{timestamp}.pkl', 'wb') as f:\n",
    "    pickle.dump(tl_results['best_history'], f)\n",
    "print(f\"✓ Best model training history saved to: best_model_history_{timestamp}.pkl\")\n",
    "\n",
    "# 6. Save training histories for top 5\n",
    "for i, history in enumerate(top5_histories, 1):\n",
    "    with open(f'{results_dir}/rank_{i:02d}_history_{timestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "print(f\"✓ Top 5 training histories saved\")\n",
    "\n",
    "# 7. Save loss curves data as CSV for easy access\n",
    "for i, history in enumerate(top5_histories, 1):\n",
    "    loss_df = pd.DataFrame({\n",
    "        'Epoch': range(1, len(history['train_loss']) + 1),\n",
    "        'Train_Loss': history['train_loss'],\n",
    "        'Val_Loss': history['val_loss'],\n",
    "        'Train_Acc': history['train_acc'],\n",
    "        'Val_Acc': history['val_acc'],\n",
    "    })\n",
    "    loss_df.to_csv(f'{results_dir}/rank_{i:02d}_loss_curves_{timestamp}.csv', index=False)\n",
    "print(f\"✓ Loss curves data saved as CSV for all top 5\")\n",
    "\n",
    "# 8. Save summary report\n",
    "total_combinations = len(tl_results['results'])\n",
    "summary = {\n",
    "    'model': 'Transfer Learning (MobileNetV2)',\n",
    "    'timestamp': timestamp,\n",
    "    'total_combinations': int(total_combinations),\n",
    "    'best_val_accuracy': float(tl_results['best_val_acc']),\n",
    "    'best_test_accuracy': float(tl_results['best_test_acc']),\n",
    "    'best_hyperparameters': tl_results['best_params'],\n",
    "    'top5_accuracies': tl_top5['Val_Acc'].tolist(),\n",
    "}\n",
    "\n",
    "with open(f'{results_dir}/summary_report_{timestamp}.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4, default=str)\n",
    "print(f\"✓ Summary report saved to: summary_report_{timestamp}.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"All results saved to: {results_dir}/\".center(80))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ed6cc",
   "metadata": {},
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4503e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSFER LEARNING (MOBILENETV2) - TUNING RESULTS SUMMARY\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nBest Model Performance:\")\n",
    "print(f\"  Validation Accuracy: {tl_results['best_val_acc']:.4f}\")\n",
    "print(f\"  Test Accuracy: {tl_results['best_test_acc']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for key, value in tl_results['best_params'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nTop 5 Validation Accuracies:\")\n",
    "for rank, acc in enumerate(tl_top5['Val_Acc'].values, 1):\n",
    "    print(f\"  Rank {rank}: {acc:.4f}\")\n",
    "\n",
    "total_combinations = len(tl_results['results'])\n",
    "print(f\"\\nTotal tuning combinations evaluated: {total_combinations}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a007c",
   "metadata": {},
   "source": [
    "## 6. Best Model Evaluation - Classification Report & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "from datetime import datetime\n",
    "\n",
    "# Load best model and generate predictions\n",
    "best_model = tl_results['best_model']\n",
    "best_model.eval()\n",
    "\n",
    "# Debug: Check X_test shape\n",
    "print(f\"Original X_test shape: {X_test.shape}\")\n",
    "\n",
    "# X_test is already 3-channel from preprocessing: (N, 3, 52, 52)\n",
    "# No need to convert - use directly\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "print(f\"X_test_tensor shape for MobileNet: {X_test_tensor.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = best_model(X_test_tensor)\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Generate classification report\n",
    "class_names = [f'Class_{i:02d}' for i in range(38)]\n",
    "report = classification_report(y_test, predictions, target_names=class_names, digits=4)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION REPORT - Transfer Learning BEST MODEL (All 38 Classes)\")\n",
    "print(\"=\"*80)\n",
    "print(report)\n",
    "\n",
    "# Save classification report\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_path = f'classification_report_{timestamp}.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"CLASSIFICATION REPORT - Transfer Learning BEST MODEL (All 38 Classes)\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(report)\n",
    "print(f\"\\n✓ Classification report saved to: {report_path}\")\n",
    "\n",
    "# Compute and visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(f\"\\nConfusion Matrix Shape: {cm.shape}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax, \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Confusion Matrix - Transfer Learning Best Model (38×38)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_path = f'confusion_matrix_{timestamp}.png'\n",
    "plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Confusion matrix saved to: {cm_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Extract per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, predictions, labels=range(38))\n",
    "class_accuracy = recall  # Recall is per-class accuracy\n",
    "\n",
    "# Create metrics dataframe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support,\n",
    "    'Accuracy': class_accuracy\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS METRICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Save per-class metrics\n",
    "metrics_path = f'class_wise_metrics_{timestamp}.csv'\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"\\n✓ Per-class metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, predictions):.4f}\")\n",
    "print(f\"Macro-Average Precision: {precision.mean():.4f}\")\n",
    "print(f\"Macro-Average Recall: {recall.mean():.4f}\")\n",
    "print(f\"Macro-Average F1-Score: {f1.mean():.4f}\")\n",
    "print(f\"\\nBest Performing Classes (Top 5 by F1-Score):\")\n",
    "top_5 = metrics_df.nlargest(5, 'F1-Score')[['Class', 'F1-Score', 'Recall', 'Support']]\n",
    "print(top_5.to_string(index=False))\n",
    "print(f\"\\nWorst Performing Classes (Bottom 5 by F1-Score):\")\n",
    "bottom_5 = metrics_df.nsmallest(5, 'F1-Score')[['Class', 'F1-Score', 'Recall', 'Support']]\n",
    "print(bottom_5.to_string(index=False))\n",
    "\n",
    "# Create comprehensive 4-subplot visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Subplot 1: Per-class Accuracy (Recall) with color coding\n",
    "ax1 = axes[0, 0]\n",
    "colors_acc = ['green' if x >= 0.8 else 'orange' if x >= 0.6 else 'red' for x in class_accuracy]\n",
    "bars1 = ax1.bar(range(38), class_accuracy, color=colors_acc, edgecolor='black', linewidth=0.5)\n",
    "ax1.axhline(y=class_accuracy.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean: {class_accuracy.mean():.3f}')\n",
    "ax1.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy (Recall)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Per-Class Accuracy Distribution', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(range(0, 38, 2))\n",
    "ax1.set_xticklabels([f'C{i}' for i in range(0, 38, 2)], fontsize=9)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Subplot 2: Precision vs Recall Scatter\n",
    "ax2 = axes[0, 1]\n",
    "colors_pr = ['green' if x >= 0.8 else 'orange' if x >= 0.6 else 'red' for x in class_accuracy]\n",
    "scatter = ax2.scatter(recall, precision, c=colors_pr, s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Performance')\n",
    "ax2.set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Precision vs Recall (Per-Class)', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlim([-0.05, 1.05])\n",
    "ax2.set_ylim([-0.05, 1.05])\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Subplot 3: F1-Score by Class\n",
    "ax3 = axes[1, 0]\n",
    "colors_f1 = ['green' if x >= 0.8 else 'orange' if x >= 0.6 else 'red' for x in f1]\n",
    "bars3 = ax3.bar(range(38), f1, color=colors_f1, edgecolor='black', linewidth=0.5)\n",
    "ax3.axhline(y=f1.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean: {f1.mean():.3f}')\n",
    "ax3.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('F1-Score Distribution by Class', fontsize=13, fontweight='bold')\n",
    "ax3.set_xticks(range(0, 38, 2))\n",
    "ax3.set_xticklabels([f'C{i}' for i in range(0, 38, 2)], fontsize=9)\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Subplot 4: Support Distribution (Sample Count)\n",
    "ax4 = axes[1, 1]\n",
    "bars4 = ax4.bar(range(38), support, color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "ax4.axhline(y=support.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {support.mean():.0f}')\n",
    "ax4.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Class Distribution in Test Set (Support)', fontsize=13, fontweight='bold')\n",
    "ax4.set_xticks(range(0, 38, 2))\n",
    "ax4.set_xticklabels([f'C{i}' for i in range(0, 38, 2)], fontsize=9)\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "accuracy_path = f'class_wise_accuracy_{timestamp}.png'\n",
    "plt.savefig(accuracy_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Class-wise accuracy visualization saved to: {accuracy_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE - All results saved successfully\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
