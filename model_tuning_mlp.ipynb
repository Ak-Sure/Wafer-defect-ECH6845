{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e1d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "sys.path.insert(0, os.getcwd())\n",
    "from data_loading import WaferDataLoader\n",
    "from utility import (setup_model_and_loaders, hyperparameter_tuning, \n",
    "                     evaluate_model, train_model)\n",
    "from models import MLP\n",
    "from config import MLP_TUNING_GRID\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✓ All libraries imported successfully!\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272c862",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2074e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using WaferDataLoader\n",
    "print(\"Loading wafer defect dataset...\")\n",
    "loader = WaferDataLoader()\n",
    "\n",
    "print(f\"✓ Dataset loaded successfully!\")\n",
    "print(f\"X shape: {loader.X.shape}\")\n",
    "print(f\"y shape: {loader.y.shape}\")\n",
    "print(f\"Number of classes: {loader.num_classes}\")\n",
    "\n",
    "# Get the data\n",
    "X = loader.X.astype('float32')\n",
    "y = loader.y\n",
    "\n",
    "# Normalize data\n",
    "X_min, X_max = X.min(), X.max()\n",
    "X_normalized = (X - X_min) / (X_max - X_min) if X_max > X_min else X\n",
    "\n",
    "print(f\"\\n✓ Normalization:\")\n",
    "print(f\"  Original range: [{X_min}, {X_max}]\")\n",
    "print(f\"  Normalized range: [{X_normalized.min():.4f}, {X_normalized.max():.4f}]\")\n",
    "\n",
    "# Split into train/val/test (70% / 15% / 15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_normalized, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data split complete:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(y)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(y)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a2db6",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tuning grid\n",
    "print(\"=\"*80)\n",
    "print(\"MLP - HYPERPARAMETER TUNING GRID\".center(80))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTuning Grid:\")\n",
    "for param, values in MLP_TUNING_GRID.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in MLP_TUNING_GRID.values()])\n",
    "print(f\"\\nTotal combinations to evaluate: {total_combinations}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b160f",
   "metadata": {},
   "source": [
    "## 3. Run Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Hyperparameter Tuning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING HYPERPARAMETER TUNING\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "mlp_results = hyperparameter_tuning(\n",
    "    MLP, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "    param_grid=MLP_TUNING_GRID,\n",
    "    input_size=2704,\n",
    "    num_classes=38,\n",
    "    device=str(device),\n",
    "    num_epochs=25,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TUNING COMPLETE\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract top 5\n",
    "mlp_top5 = mlp_results['summary_df'].head(5).copy()\n",
    "print(\"\\nTop 5 Configurations:\")\n",
    "print(mlp_top5[['learning_rate', 'batch_size', 'hidden_sizes', 'dropout', 'num_epochs', 'optimizer', 'Val_Acc', 'Test_Acc']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58e8cd",
   "metadata": {},
   "source": [
    "## 4. Validation Loss Curves - Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e9a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain top 5 models to get training histories\n",
    "print(\"\\nRetraining top 5 models to generate loss curves...\")\n",
    "\n",
    "def retrain_with_history(model_class, X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                         params, input_size, num_classes, device_str):\n",
    "    \"\"\"Retrain a model with given params and return training history\"\"\"\n",
    "    try:\n",
    "        # Extract parameters\n",
    "        learning_rate = params.get('learning_rate', 0.001)\n",
    "        batch_size = params.get('batch_size', 64)\n",
    "        optimizer_type = params.get('optimizer', 'adam')\n",
    "        epochs_to_train = params.get('num_epochs', 20)\n",
    "        \n",
    "        # Build model_kwargs for architecture parameters\n",
    "        model_kwargs = {}\n",
    "        if 'hidden_sizes' in params:\n",
    "            hidden_sizes = params['hidden_sizes']\n",
    "            if isinstance(hidden_sizes, str):\n",
    "                hidden_sizes = hidden_sizes.strip('[]').split(',')\n",
    "                hidden_sizes = [int(h.strip()) for h in hidden_sizes]\n",
    "            model_kwargs['hidden_sizes'] = hidden_sizes\n",
    "        \n",
    "        if 'dropout' in params:\n",
    "            model_kwargs['dropout'] = params['dropout']\n",
    "        \n",
    "        # Setup model and loaders\n",
    "        setup_result = setup_model_and_loaders(\n",
    "            model_class, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "            input_size=input_size, num_classes=num_classes, device=device_str,\n",
    "            batch_size=batch_size, model_kwargs=model_kwargs, verbose=False\n",
    "        )\n",
    "        \n",
    "        model = setup_result['model']\n",
    "        train_loader = setup_result['train_loader']\n",
    "        val_loader = setup_result['val_loader']\n",
    "        \n",
    "        # Setup optimizer\n",
    "        if optimizer_type.lower() == 'adam':\n",
    "            opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            opt = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        \n",
    "        # Train\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        history = train_model(\n",
    "            model, train_loader, val_loader, criterion, opt,\n",
    "            num_epochs=epochs_to_train, device=device_str, patience=5\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get histories for top 5\n",
    "top5_histories = []\n",
    "for i, (_, row) in enumerate(mlp_top5.iterrows(), 1):\n",
    "    print(f\"  Retraining rank {i}/5...\")\n",
    "    \n",
    "    # Extract parameters from row\n",
    "    params = {\n",
    "        'learning_rate': row['learning_rate'],\n",
    "        'batch_size': int(row['batch_size']),\n",
    "        'optimizer': row['optimizer'],\n",
    "        'num_epochs': int(row['num_epochs']),\n",
    "        'hidden_sizes': row['hidden_sizes'],\n",
    "        'dropout': row['dropout'],\n",
    "    }\n",
    "    \n",
    "    history = retrain_with_history(\n",
    "        MLP, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        params, input_size=2704, num_classes=38, device_str=str(device)\n",
    "    )\n",
    "    if history:\n",
    "        top5_histories.append(history)\n",
    "\n",
    "print(f\"✓ Successfully generated {len(top5_histories)} training histories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation loss curves for top 5\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('Multi-Layer Perceptron - Top 5 Configurations\\nTraining vs Validation Loss', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, history in enumerate(top5_histories):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "    ax.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "    \n",
    "    rank = idx + 1\n",
    "    val_acc = mlp_top5.iloc[idx]['Val_Acc']\n",
    "    ax.set_title(f'Rank {rank} - Val Acc: {val_acc:.4f}', fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[1, 2].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mlp_validation_loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Loss curves plot saved as 'mlp_validation_loss_curves.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841bb98",
   "metadata": {},
   "source": [
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = 'mlp_results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Save summary CSV with all results\n",
    "mlp_results['summary_df'].to_csv(f'{results_dir}/all_results_{timestamp}.csv', index=False)\n",
    "print(f\"✓ All results saved to: all_results_{timestamp}.csv\")\n",
    "\n",
    "# 2. Save top 5 CSV\n",
    "mlp_top5.to_csv(f'{results_dir}/top5_results_{timestamp}.csv', index=False)\n",
    "print(f\"✓ Top 5 results saved to: top5_results_{timestamp}.csv\")\n",
    "\n",
    "# 3. Save best model info\n",
    "best_model_info = {\n",
    "    'best_params': mlp_results['best_params'],\n",
    "    'best_val_acc': float(mlp_results['best_val_acc']),\n",
    "    'best_test_acc': float(mlp_results['best_test_acc']),\n",
    "}\n",
    "\n",
    "with open(f'{results_dir}/best_model_{timestamp}.json', 'w') as f:\n",
    "    json.dump(best_model_info, f, indent=4, default=str)\n",
    "print(f\"✓ Best model info saved to: best_model_{timestamp}.json\")\n",
    "\n",
    "# 4. Save best model itself\n",
    "torch.save(mlp_results['best_model'].state_dict(), \n",
    "           f'{results_dir}/best_model_weights_{timestamp}.pt')\n",
    "print(f\"✓ Best model weights saved to: best_model_weights_{timestamp}.pt\")\n",
    "\n",
    "# 5. Save training history for best model\n",
    "with open(f'{results_dir}/best_model_history_{timestamp}.pkl', 'wb') as f:\n",
    "    pickle.dump(mlp_results['best_history'], f)\n",
    "print(f\"✓ Best model training history saved to: best_model_history_{timestamp}.pkl\")\n",
    "\n",
    "# 6. Save training histories for top 5\n",
    "for i, history in enumerate(top5_histories, 1):\n",
    "    with open(f'{results_dir}/rank_{i:02d}_history_{timestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "print(f\"✓ Top 5 training histories saved\")\n",
    "\n",
    "# 7. Save loss curves data as CSV for easy access\n",
    "for i, history in enumerate(top5_histories, 1):\n",
    "    loss_df = pd.DataFrame({\n",
    "        'Epoch': range(1, len(history['train_loss']) + 1),\n",
    "        'Train_Loss': history['train_loss'],\n",
    "        'Val_Loss': history['val_loss'],\n",
    "        'Train_Acc': history['train_acc'],\n",
    "        'Val_Acc': history['val_acc'],\n",
    "    })\n",
    "    loss_df.to_csv(f'{results_dir}/rank_{i:02d}_loss_curves_{timestamp}.csv', index=False)\n",
    "print(f\"✓ Loss curves data saved as CSV for all top 5\")\n",
    "\n",
    "# 8. Save summary report\n",
    "summary = {\n",
    "    'model': 'MLP',\n",
    "    'timestamp': timestamp,\n",
    "    'total_combinations': int(total_combinations),\n",
    "    'best_val_accuracy': float(mlp_results['best_val_acc']),\n",
    "    'best_test_accuracy': float(mlp_results['best_test_acc']),\n",
    "    'best_hyperparameters': mlp_results['best_params'],\n",
    "    'top5_accuracies': mlp_top5['Val_Acc'].tolist(),\n",
    "}\n",
    "\n",
    "with open(f'{results_dir}/summary_report_{timestamp}.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4, default=str)\n",
    "print(f\"✓ Summary report saved to: summary_report_{timestamp}.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"All results saved to: {results_dir}/\".center(80))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5bbf2",
   "metadata": {},
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9b028",
   "metadata": {},
   "source": [
    "## 6. Best Model Evaluation - Classification Report & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "from datetime import datetime\n",
    "\n",
    "# Load best model and generate predictions\n",
    "best_model = mlp_results['best_model']\n",
    "best_model.eval()\n",
    "\n",
    "# Generate predictions on test set\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = best_model(X_test_tensor)\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Generate classification report\n",
    "class_names = [f'Class_{i:02d}' for i in range(38)]\n",
    "report = classification_report(y_test, predictions, target_names=class_names, digits=4)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION REPORT - MLP BEST MODEL (All 38 Classes)\")\n",
    "print(\"=\"*80)\n",
    "print(report)\n",
    "\n",
    "# Save classification report\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_path = f'classification_report_{timestamp}.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"CLASSIFICATION REPORT - MLP BEST MODEL (All 38 Classes)\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(report)\n",
    "print(f\"\\n✓ Classification report saved to: {report_path}\")\n",
    "\n",
    "# Compute and visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(f\"\\nConfusion Matrix Shape: {cm.shape}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax, \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Confusion Matrix - MLP Best Model (38×38)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_path = f'confusion_matrix_{timestamp}.png'\n",
    "plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Confusion matrix saved to: {cm_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Extract per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, predictions, labels=range(38))\n",
    "class_accuracy = recall  # Recall is per-class accuracy\n",
    "\n",
    "# Create metrics dataframe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support,\n",
    "    'Accuracy': class_accuracy\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS METRICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Save per-class metrics\n",
    "metrics_path = f'class_wise_metrics_{timestamp}.csv'\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"\\n✓ Per-class metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, predictions):.4f}\")\n",
    "print(f\"Macro-Average Precision: {precision.mean():.4f}\")\n",
    "print(f\"Macro-Average Recall: {recall.mean():.4f}\")\n",
    "print(f\"Macro-Average F1-Score: {f1.mean():.4f}\")\n",
    "print(f\"\\nBest Performing Classes (Top 5 by F1-Score):\")\n",
    "top_5 = metrics_df.nlargest(5, 'F1-Score')[['Class', 'F1-Score', 'Recall', 'Support']]\n",
    "print(top_5.to_string(index=False))\n",
    "print(f\"\\nWorst Performing Classes (Bottom 5 by F1-Score):\")\n",
    "bottom_5 = metrics_df.nsmallest(5, 'F1-Score')[['Class', 'F1-Score', 'Recall', 'Support']]\n",
    "print(bottom_5.to_string(index=False))\n",
    "\n",
    "# Create comprehensive 4-subplot visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Subplot 1: Per-class Accuracy (Recall) with color coding\n",
    "ax1 = axes[0, 0]\n",
    "colors_acc = ['green' if x >= 0.8 else 'orange' if x >= 0.6 else 'red' for x in class_accuracy]\n",
    "bars1 = ax1.bar(range(38), class_accuracy, color=colors_acc, edgecolor='black', linewidth=0.5)\n",
    "ax1.axhline(y=class_accuracy.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean: {class_accuracy.mean():.3f}')\n",
    "ax1.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy (Recall)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Per-Class Accuracy Distribution', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(range(0, 38, 2))\n",
    "ax1.set_xticklabels([f'C{i}' for i in range(0, 38, 2)], fontsize=9)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Subplot 2: Precision vs Recall Scatter\n",
    "ax2 = axes[0, 1]\n",
    "colors_pr = ['green' if x >= 0.8 else 'orange' if x >= 0.6 else 'red' for x in class_accuracy]\n",
    "scatter = ax2.scatter(recall, precision, c=colors_pr, s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Performance')\n",
    "ax2.set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Precision vs Recall (Per-Class)', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlim([-0.05, 1.05])\n",
    "ax2.set_ylim([-0.05, 1.05])\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Subplot 3: F1-Score by Class\n",
    "ax3 = axes[1, 0]\n",
    "colors_f1 = ['green' if x >= 0.8 else 'orange' if x >= 0.6 else 'red' for x in f1]\n",
    "bars3 = ax3.bar(range(38), f1, color=colors_f1, edgecolor='black', linewidth=0.5)\n",
    "ax3.axhline(y=f1.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean: {f1.mean():.3f}')\n",
    "ax3.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('F1-Score Distribution by Class', fontsize=13, fontweight='bold')\n",
    "ax3.set_xticks(range(0, 38, 2))\n",
    "ax3.set_xticklabels([f'C{i}' for i in range(0, 38, 2)], fontsize=9)\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Subplot 4: Support Distribution (Sample Count)\n",
    "ax4 = axes[1, 1]\n",
    "bars4 = ax4.bar(range(38), support, color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "ax4.axhline(y=support.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {support.mean():.0f}')\n",
    "ax4.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Class Distribution in Test Set (Support)', fontsize=13, fontweight='bold')\n",
    "ax4.set_xticks(range(0, 38, 2))\n",
    "ax4.set_xticklabels([f'C{i}' for i in range(0, 38, 2)], fontsize=9)\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "accuracy_path = f'class_wise_accuracy_{timestamp}.png'\n",
    "plt.savefig(accuracy_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Class-wise accuracy visualization saved to: {accuracy_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE - All results saved successfully\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-LAYER PERCEPTRON - TUNING RESULTS SUMMARY\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nBest Model Performance:\")\n",
    "print(f\"  Validation Accuracy: {mlp_results['best_val_acc']:.4f}\")\n",
    "print(f\"  Test Accuracy: {mlp_results['best_test_acc']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for key, value in mlp_results['best_params'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nTop 5 Validation Accuracies:\")\n",
    "for rank, acc in enumerate(mlp_top5['Val_Acc'].values, 1):\n",
    "    print(f\"  Rank {rank}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal tuning combinations evaluated: {total_combinations}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
