{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28dac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d5646",
   "metadata": {},
   "source": [
    "## 1. Load Best Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from each model's directory\n",
    "results_dirs = {\n",
    "    'SimpleNN': 'simplenn_results',\n",
    "    'MLP': 'mlp_results',\n",
    "    'CNN': 'cnn_results',\n",
    "    'Transfer Learning': 'transfer_learning_results'\n",
    "}\n",
    "\n",
    "# Function to load the most recent summary report\n",
    "def load_latest_summary(results_dir):\n",
    "    \"\"\"Load the most recent summary report from results directory\"\"\"\n",
    "    try:\n",
    "        # Find all summary_report files\n",
    "        files = [f for f in os.listdir(results_dir) if f.startswith('summary_report_')]\n",
    "        if not files:\n",
    "            print(f\"  âœ— No summary reports found in {results_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Sort by timestamp (most recent first)\n",
    "        latest_file = sorted(files, reverse=True)[0]\n",
    "        \n",
    "        with open(os.path.join(results_dir, latest_file), 'r') as f:\n",
    "            summary = json.load(f)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error loading summary from {results_dir}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load all summaries\n",
    "print(\"Loading model results...\\n\")\n",
    "all_summaries = {}\n",
    "for model_name, results_dir in results_dirs.items():\n",
    "    print(f\"  Loading {model_name}...\")\n",
    "    summary = load_latest_summary(results_dir)\n",
    "    if summary:\n",
    "        all_summaries[model_name] = summary\n",
    "        print(f\"    âœ“ Val Acc: {summary['best_val_accuracy']:.4f}, Test Acc: {summary['best_test_accuracy']:.4f}\")\n",
    "    else:\n",
    "        print(f\"    âœ— Failed to load\")\n",
    "\n",
    "print(f\"\\nâœ“ Successfully loaded {len(all_summaries)}/4 models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887f931",
   "metadata": {},
   "source": [
    "## 2. Performance Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf57ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, summary in all_summaries.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Val Accuracy': summary['best_val_accuracy'],\n",
    "        'Test Accuracy': summary['best_test_accuracy'],\n",
    "        'Tuning Combinations': summary['total_combinations'],\n",
    "        'Top 5 Mean Val Acc': np.mean(summary['top5_accuracies']),\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTER-MODEL COMPARISON\".center(80))\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPerformance Metrics (sorted by Test Accuracy):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed3b96c",
   "metadata": {},
   "source": [
    "## 3. Best Hyperparameters Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ed656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best hyperparameters for each model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST HYPERPARAMETERS BY MODEL\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, summary in all_summaries.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Best Validation Accuracy: {summary['best_val_accuracy']:.4f}\")\n",
    "    print(f\"  Best Test Accuracy: {summary['best_test_accuracy']:.4f}\")\n",
    "    print(f\"  \\n  Hyperparameters:\")\n",
    "    for key, value in summary['best_hyperparameters'].items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1ac39",
   "metadata": {},
   "source": [
    "## 4. Accuracy Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f878b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Inter-Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Validation Accuracy Comparison\n",
    "ax = axes[0, 0]\n",
    "models = list(all_summaries.keys())\n",
    "val_accs = [all_summaries[m]['best_val_accuracy'] for m in models]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "bars1 = ax.bar(models, val_accs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Validation Accuracy', fontweight='bold')\n",
    "ax.set_title('Validation Accuracy Comparison', fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, acc in zip(bars1, val_accs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 2. Test Accuracy Comparison\n",
    "ax = axes[0, 1]\n",
    "test_accs = [all_summaries[m]['best_test_accuracy'] for m in models]\n",
    "bars2 = ax.bar(models, test_accs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Test Accuracy', fontweight='bold')\n",
    "ax.set_title('Test Accuracy Comparison', fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, acc in zip(bars2, test_accs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 3. Top 5 Average Validation Accuracy\n",
    "ax = axes[1, 0]\n",
    "top5_avgs = [np.mean(all_summaries[m]['top5_accuracies']) for m in models]\n",
    "bars3 = ax.bar(models, top5_avgs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Average Validation Accuracy', fontweight='bold')\n",
    "ax.set_title('Top 5 Average Validation Accuracy', fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, avg in zip(bars3, top5_avgs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{avg:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 4. Accuracy Gap (Val - Test)\n",
    "ax = axes[1, 1]\n",
    "gaps = [all_summaries[m]['best_val_accuracy'] - all_summaries[m]['best_test_accuracy'] for m in models]\n",
    "bars4 = ax.bar(models, gaps, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Accuracy Gap (Val - Test)', fontweight='bold')\n",
    "ax.set_title('Overfitting Analysis (Gap)', fontweight='bold')\n",
    "ax.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, gap in zip(bars4, gaps):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{gap:.4f}', ha='center', va='bottom' if gap >= 0 else 'top', fontweight='bold')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('inter_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Comparison plot saved as 'inter_model_comparison.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a5216",
   "metadata": {},
   "source": [
    "## 5. Loss Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79281ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load training history\n",
    "def load_best_history(results_dir):\n",
    "    \"\"\"Load the best model's training history\"\"\"\n",
    "    try:\n",
    "        files = [f for f in os.listdir(results_dir) if f.startswith('best_model_history_')]\n",
    "        if not files:\n",
    "            return None\n",
    "        latest_file = sorted(files, reverse=True)[0]\n",
    "        with open(os.path.join(results_dir, latest_file), 'rb') as f:\n",
    "            history = pickle.load(f)\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading history from {results_dir}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load histories for all models\n",
    "print(\"\\nLoading training histories...\")\n",
    "histories = {}\n",
    "for model_name, results_dir in results_dirs.items():\n",
    "    history = load_best_history(results_dir)\n",
    "    if history:\n",
    "        histories[model_name] = history\n",
    "        print(f\"  âœ“ {model_name}: {len(history['train_loss'])} epochs\")\n",
    "    else:\n",
    "        print(f\"  âœ— {model_name}: Failed to load\")\n",
    "\n",
    "print(f\"\\nâœ“ Successfully loaded {len(histories)}/4 histories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss curves for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Training Loss Curves - All Models (Best Configuration)', fontsize=16, fontweight='bold')\n",
    "\n",
    "model_names = list(histories.keys())\n",
    "colors_loss = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "# Individual plots for each model\n",
    "for idx, (model_name, history) in enumerate(histories.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2.5, marker='o', markersize=5)\n",
    "    ax.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2.5, marker='s', markersize=5)\n",
    "    \n",
    "    ax.set_title(f'{model_name}', fontweight='bold', fontsize=13)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='upper right', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_curves_all_models.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Loss curves plot saved as 'loss_curves_all_models.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overlaid loss curves for comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Normalize epochs to 0-1 for comparison (since models train for different durations)\n",
    "colors_map = {'SimpleNN': 'blue', 'MLP': 'green', 'CNN': 'red', 'Transfer Learning': 'purple'}\n",
    "\n",
    "# Training Loss\n",
    "for model_name, history in histories.items():\n",
    "    epochs_normalized = np.linspace(0, 1, len(history['train_loss']))\n",
    "    ax1.plot(epochs_normalized, history['train_loss'], label=model_name, \n",
    "             color=colors_map.get(model_name, 'black'), linewidth=2.5, marker='o', markersize=4)\n",
    "\n",
    "ax1.set_title('Training Loss Comparison (Normalized Epochs)', fontweight='bold', fontsize=13)\n",
    "ax1.set_xlabel('Normalized Epoch')\n",
    "ax1.set_ylabel('Training Loss')\n",
    "ax1.legend(fontsize=11, loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "for model_name, history in histories.items():\n",
    "    epochs_normalized = np.linspace(0, 1, len(history['val_loss']))\n",
    "    ax2.plot(epochs_normalized, history['val_loss'], label=model_name, \n",
    "             color=colors_map.get(model_name, 'black'), linewidth=2.5, marker='s', markersize=4)\n",
    "\n",
    "ax2.set_title('Validation Loss Comparison (Normalized Epochs)', fontweight='bold', fontsize=13)\n",
    "ax2.set_xlabel('Normalized Epoch')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.legend(fontsize=11, loc='best')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_curves_comparison_normalized.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Normalized loss curves plot saved as 'loss_curves_comparison_normalized.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417cbb3",
   "metadata": {},
   "source": [
    "## 6. Final Ranking and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d455cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final ranking\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RANKING - BEST TO WORST\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "ranking = comparison_df.sort_values('Test Accuracy', ascending=False).reset_index(drop=True)\n",
    "ranking['Rank'] = range(1, len(ranking) + 1)\n",
    "\n",
    "print(\"\\n\" + ranking[['Rank', 'Model', 'Val Accuracy', 'Test Accuracy', 'Top 5 Mean Val Acc']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = ranking.iloc[0]\n",
    "print(f\"\\nðŸ† BEST OVERALL MODEL: {best_model['Model']}\")\n",
    "print(f\"   - Test Accuracy: {best_model['Test Accuracy']:.4f}\")\n",
    "print(f\"   - Validation Accuracy: {best_model['Val Accuracy']:.4f}\")\n",
    "print(f\"   - Recommendation: Use this model for production deployment\")\n",
    "\n",
    "# Check for overfitting\n",
    "gaps = comparison_df.copy()\n",
    "gaps['Gap'] = gaps['Val Accuracy'] - gaps['Test Accuracy']\n",
    "best_generalization = gaps.sort_values('Gap').iloc[0]\n",
    "\n",
    "print(f\"\\nðŸ’ª BEST GENERALIZATION (Lowest Overfitting): {best_generalization['Model']}\")\n",
    "print(f\"   - Val-Test Gap: {best_generalization['Gap']:.4f}\")\n",
    "print(f\"   - Recommendation: More robust to unseen data\")\n",
    "\n",
    "# Most efficient\n",
    "efficient = ranking.sort_values('Tuning Combinations').iloc[0]\n",
    "print(f\"\\nâš¡ MOST EFFICIENT: {efficient['Model']}\")\n",
    "print(f\"   - Tuning Combinations: {int(efficient['Tuning Combinations'])}\")\n",
    "print(f\"   - Test Accuracy: {efficient['Test Accuracy']:.4f}\")\n",
    "print(f\"   - Recommendation: Faster tuning process\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0612b",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive comparison report\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "report = {\n",
    "    'timestamp': timestamp,\n",
    "    'comparison': comparison_df.to_dict('records'),\n",
    "    'best_model': {\n",
    "        'name': best_model['Model'],\n",
    "        'test_accuracy': float(best_model['Test Accuracy']),\n",
    "        'val_accuracy': float(best_model['Val Accuracy']),\n",
    "        'hyperparameters': all_summaries[best_model['Model']]['best_hyperparameters']\n",
    "    },\n",
    "    'all_model_details': {model: summary for model, summary in all_summaries.items()}\n",
    "}\n",
    "\n",
    "with open(f'inter_model_comparison_report_{timestamp}.json', 'w') as f:\n",
    "    json.dump(report, f, indent=4, default=str)\n",
    "\n",
    "print(f\"\\nâœ“ Comprehensive comparison report saved to: inter_model_comparison_report_{timestamp}.json\")\n",
    "\n",
    "# Save comparison table as CSV\n",
    "comparison_df.to_csv(f'inter_model_comparison_{timestamp}.csv', index=False)\n",
    "print(f\"âœ“ Comparison table saved to: inter_model_comparison_{timestamp}.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"INTER-MODEL COMPARISON COMPLETE!\".center(80))\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1ea8a",
   "metadata": {},
   "source": [
    "## 5. Class-wise Classification Reports & Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cd8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and best models for evaluation\n",
    "import sys\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from data_loading import WaferDataLoader\n",
    "from models import SimpleNN, MLP, WaferCNN, WaferResNet18  # Updated to ResNet18\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading dataset and best models...\\n\")\n",
    "\n",
    "# Load data\n",
    "loader = WaferDataLoader()\n",
    "X = loader.X.astype('float32')\n",
    "y = loader.y\n",
    "\n",
    "# Normalize\n",
    "X_min, X_max = X.min(), X.max()\n",
    "X_normalized = (X - X_min) / (X_max - X_min) if X_max > X_min else X\n",
    "\n",
    "# Prepare data for each model type\n",
    "X_flat = X_normalized.reshape(X_normalized.shape[0], -1)  # Flattened for SimpleNN/MLP\n",
    "X_2d = X_normalized.reshape(-1, 52, 52)  # 2D for CNN\n",
    "X_3ch = np.repeat(X_2d[:, np.newaxis, :, :], 3, axis=1)  # 3-channel for Transfer Learning\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_flat, X_temp_flat, y_train, y_temp = train_test_split(\n",
    "    X_flat, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val_flat, X_test_flat, y_val, y_test = train_test_split(\n",
    "    X_temp_flat, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Use same split indices as flat data to ensure consistency\n",
    "X_train_2d, X_temp_2d, _, _ = train_test_split(\n",
    "    X_2d, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val_2d, X_test_2d, _, _ = train_test_split(\n",
    "    X_temp_2d, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Add channel dimension for CNN: (N, 52, 52) -> (N, 1, 52, 52)\n",
    "X_test_2d = X_test_2d[:, np.newaxis, :, :]\n",
    "\n",
    "# Use same split indices as other data to ensure consistency\n",
    "X_train_3ch, X_temp_3ch, _, _ = train_test_split(\n",
    "    X_3ch, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val_3ch, X_test_3ch, _, _ = train_test_split(\n",
    "    X_temp_3ch, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Load best model weights\n",
    "def load_best_model(model_class, results_dir, model_params=None):\n",
    "    \"\"\"Load the best model from results directory\"\"\"\n",
    "    try:\n",
    "        files = sorted([f for f in os.listdir(results_dir) if f.startswith('best_model_weights_')], reverse=True)\n",
    "        if not files:\n",
    "            print(f\"    âœ— No model weights found in {results_dir}\")\n",
    "            return None\n",
    "        \n",
    "        model_file = os.path.join(results_dir, files[0])\n",
    "        if model_params:\n",
    "            model = model_class(**model_params)\n",
    "        else:\n",
    "            model = model_class()\n",
    "        \n",
    "        # Load state dict and handle size mismatches for Transfer Learning\n",
    "        state_dict = torch.load(model_file, map_location=device)\n",
    "        \n",
    "        # Special handling for WaferResNet18 classifier layer size mismatch\n",
    "        if model_class.__name__ == 'WaferResNet18':\n",
    "            # Get the expected classifier size from the current model\n",
    "            expected_out_features = model.resnet.fc[1].out_features  # fc is Sequential(Dropout, Linear)\n",
    "            \n",
    "            # Check if the saved classifier has different size\n",
    "            saved_classifier_weight = state_dict.get('resnet.fc.1.weight')\n",
    "            if saved_classifier_weight is not None:\n",
    "                saved_out_features = saved_classifier_weight.shape[0]\n",
    "                \n",
    "                if saved_out_features != expected_out_features:\n",
    "                    print(f\"    âœ— ERROR: Saved classifier has {saved_out_features} classes, expected {expected_out_features}\")\n",
    "                    print(f\"    âœ— The Transfer Learning model was not properly trained for 38 classes.\")\n",
    "                    print(f\"    âœ— Please re-run model_tuning_transfer_learning.ipynb to train a proper model.\")\n",
    "                    print(f\"    âœ— Skipping Transfer Learning model from comparison.\")\n",
    "                    return None  # Return None to skip this model\n",
    "                else:\n",
    "                    # Normal loading if sizes match\n",
    "                    model.load_state_dict(state_dict)\n",
    "            else:\n",
    "                # If no classifier found, skip\n",
    "                print(f\"    âœ— No classifier weights found in saved model\")\n",
    "                return None\n",
    "        else:\n",
    "            # Normal loading for other models\n",
    "            model.load_state_dict(state_dict)\n",
    "        \n",
    "        model.eval()\n",
    "        return model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {results_dir}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load models with appropriate parameters\n",
    "models_to_load = {\n",
    "    'SimpleNN': (SimpleNN, 'simplenn_results', {'input_size': 2704, 'num_classes': 38}),\n",
    "    'MLP': (MLP, 'mlp_results', {'input_size': 2704, 'num_classes': 38}),\n",
    "    'CNN': (WaferCNN, 'cnn_results', {'num_classes': 38}),\n",
    "    'Transfer Learning': (WaferResNet18, 'transfer_learning_results', {'num_classes': 38}),  # Updated to ResNet18\n",
    "}\n",
    "\n",
    "loaded_models = {}\n",
    "for model_name, (model_class, results_dir, params) in models_to_load.items():\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    model = load_best_model(model_class, results_dir, params)\n",
    "    if model:\n",
    "        loaded_models[model_name] = model\n",
    "        print(f\"  âœ“ Loaded successfully\")\n",
    "    else:\n",
    "        print(f\"  âœ— Failed to load - will be excluded from comparison\")\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(loaded_models)}/{len(models_to_load)} models\")\n",
    "\n",
    "if len(loaded_models) < len(models_to_load):\n",
    "    print(\"\\nâš ï¸  Note: Some models could not be loaded. They will be excluded from the comparison.\")\n",
    "    print(\"   To include Transfer Learning, please re-run model_tuning_transfer_learning.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all models\n",
    "def get_predictions(model, X_test, model_type='default'):\n",
    "    \"\"\"Get predictions from a model\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Handle different input formats\n",
    "        if model_type == 'flat':\n",
    "            # For SimpleNN/MLP: (N, 2704)\n",
    "            X_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        elif model_type == '2d':\n",
    "            # For CNN: (N, 1, 52, 52) - already has channel dimension\n",
    "            X_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        elif model_type == '3ch':\n",
    "            # For Transfer Learning: (N, 3, 52, 52)\n",
    "            X_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        else:\n",
    "            X_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        \n",
    "        print(f\"  Input tensor shape: {X_tensor.shape}\")\n",
    "        outputs = model(X_tensor)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "print(\"Generating predictions on test set...\\n\")\n",
    "\n",
    "all_predictions = {}\n",
    "test_data_map = {\n",
    "    'SimpleNN': (X_test_flat, y_test, 'flat'),\n",
    "    'MLP': (X_test_flat, y_test, 'flat'),\n",
    "    'CNN': (X_test_2d, y_test, '2d'),\n",
    "    'Transfer Learning': (X_test_3ch, y_test, '3ch'),\n",
    "}\n",
    "\n",
    "for model_name, model in loaded_models.items():\n",
    "    X_test, y_test_actual, model_type = test_data_map[model_name]\n",
    "    preds = get_predictions(model, X_test, model_type)\n",
    "    all_predictions[model_name] = preds\n",
    "    accuracy = accuracy_score(y_test_actual, preds)\n",
    "    print(f\"{model_name:20s} - Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Predictions generated for all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification reports for each model\n",
    "print(\"=\"*100)\n",
    "print(\"CLASS-WISE CLASSIFICATION REPORTS\".center(100))\n",
    "print(\"=\"*100)\n",
    "\n",
    "class_names = [f'Class_{i:02d}' for i in range(38)]\n",
    "all_reports = {}\n",
    "\n",
    "for model_name, predictions in all_predictions.items():\n",
    "    X_test, y_test_actual, _ = test_data_map[model_name]\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"{model_name.upper()}\".center(100))\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test_actual, predictions, target_names=class_names, digits=4)\n",
    "    all_reports[model_name] = report\n",
    "    print(report)\n",
    "    \n",
    "    # Also save to file\n",
    "    report_file = f'classification_report_{model_name.lower().replace(\" \", \"_\")}.txt'\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(f\"Classification Report: {model_name}\\n\")\n",
    "        f.write(f\"{'='*100}\\n\\n\")\n",
    "        f.write(report)\n",
    "    print(f\"  âœ“ Report saved to: {report_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dec2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize confusion matrices\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CONFUSION MATRICES\".center(100))\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, predictions) in enumerate(all_predictions.items()):\n",
    "    X_test, y_test_actual, _ = test_data_map[model_name]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test_actual, predictions)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', cbar=True, \n",
    "                ax=ax, cbar_kws={'label': 'Count'}, square=True)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_actual, predictions)\n",
    "    ax.set_title(f'{model_name} Confusion Matrix\\n(Test Accuracy: {accuracy:.4f})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Class', fontsize=12)\n",
    "    ax.set_ylabel('True Class', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices_all_models.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Confusion matrices plot saved as 'confusion_matrices_all_models.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Save individual high-resolution confusion matrices\n",
    "for model_name, predictions in all_predictions.items():\n",
    "    X_test, y_test_actual, _ = test_data_map[model_name]\n",
    "    cm = confusion_matrix(y_test_actual, predictions)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 14))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', cbar=True,\n",
    "                ax=ax, cbar_kws={'label': 'Count'}, square=True, \n",
    "                xticklabels=range(38), yticklabels=range(38))\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_actual, predictions)\n",
    "    plt.title(f'{model_name} - Detailed Confusion Matrix (Test Accuracy: {accuracy:.4f})',\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Class', fontsize=14)\n",
    "    plt.ylabel('True Class', fontsize=14)\n",
    "    \n",
    "    filename = f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}_detailed.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Detailed confusion matrix saved as '{filename}'\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-class metrics and create summary tables\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PER-CLASS PERFORMANCE METRICS SUMMARY\".center(100))\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "all_class_metrics = {}\n",
    "\n",
    "for model_name, predictions in all_predictions.items():\n",
    "    X_test, y_test_actual, _ = test_data_map[model_name]\n",
    "    \n",
    "    # Get precision, recall, f1, support for each class\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test_actual, predictions, average=None\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame\n",
    "    class_metrics_df = pd.DataFrame({\n",
    "        'Class': [f'Class_{i:02d}' for i in range(38)],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "    \n",
    "    all_class_metrics[model_name] = class_metrics_df\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 100)\n",
    "    print(class_metrics_df.to_string(index=False))\n",
    "    print(f\"\\nMacro Avg - Precision: {precision.mean():.4f}, Recall: {recall.mean():.4f}, F1: {f1.mean():.4f}\")\n",
    "    print(f\"Weighted Avg - Precision: {(precision * support / support.sum()).sum():.4f}, \"\n",
    "          f\"Recall: {(recall * support / support.sum()).sum():.4f}, \"\n",
    "          f\"F1: {(f1 * support / support.sum()).sum():.4f}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_file = f'per_class_metrics_{model_name.lower().replace(\" \", \"_\")}.csv'\n",
    "    class_metrics_df.to_csv(csv_file, index=False)\n",
    "    print(f\"  âœ“ Saved to: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations for class-wise metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for metric_idx, metric in enumerate(metrics):\n",
    "    ax = axes[metric_idx]\n",
    "    \n",
    "    x = np.arange(38)\n",
    "    width = 0.2\n",
    "    \n",
    "    for model_idx, (model_name, metrics_df) in enumerate(all_class_metrics.items()):\n",
    "        values = metrics_df[metric].values\n",
    "        ax.bar(x + (model_idx - 1.5) * width, values, width, \n",
    "               label=model_name, alpha=0.8, color=colors[model_idx])\n",
    "    \n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_title(f'{metric} by Class (All Models)', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    ax.set_xticks(range(0, 38, 4))\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Overall accuracy comparison in the 4th subplot\n",
    "ax = axes[3]\n",
    "model_names = list(all_class_metrics.keys())\n",
    "accuracies = []\n",
    "for model_name in model_names:\n",
    "    preds = all_predictions[model_name]\n",
    "    X_test, y_test_actual, _ = test_data_map[model_name]\n",
    "    acc = accuracy_score(y_test_actual, preds)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "bars = ax.bar(model_names, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Overall Test Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim([min(accuracies) - 0.05, 1.0])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nâœ“ Per-class metrics comparison plot saved as 'per_class_metrics_comparison.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fec459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary report\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "summary_report = {\n",
    "    'timestamp': timestamp,\n",
    "    'models_compared': list(all_predictions.keys()),\n",
    "    'num_classes': 38,\n",
    "    'test_set_size': len(y_test),\n",
    "}\n",
    "\n",
    "# Add accuracy and per-class metrics for each model\n",
    "for model_name in all_predictions.keys():\n",
    "    preds = all_predictions[model_name]\n",
    "    X_test, y_test_actual, _ = test_data_map[model_name]\n",
    "    \n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test_actual, preds, average=None\n",
    "    )\n",
    "    \n",
    "    summary_report[model_name] = {\n",
    "        'test_accuracy': float(accuracy_score(y_test_actual, preds)),\n",
    "        'macro_precision': float(precision.mean()),\n",
    "        'macro_recall': float(recall.mean()),\n",
    "        'macro_f1': float(f1.mean()),\n",
    "        'weighted_precision': float((precision * support / support.sum()).sum()),\n",
    "        'weighted_recall': float((recall * support / support.sum()).sum()),\n",
    "        'weighted_f1': float((f1 * support / support.sum()).sum()),\n",
    "    }\n",
    "\n",
    "# Save comprehensive report\n",
    "report_file = f'model_comparison_comprehensive_{timestamp}.json'\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=4, default=str)\n",
    "print(f\"âœ“ Comprehensive report saved to: {report_file}\")\n",
    "\n",
    "# Create markdown summary\n",
    "markdown_summary = \"# Model Comparison: Classification Reports & Confusion Matrices\\n\\n\"\n",
    "markdown_summary += f\"Generated: {timestamp}\\n\\n\"\n",
    "\n",
    "markdown_summary += \"## Overall Performance Summary\\n\\n\"\n",
    "markdown_summary += \"| Model | Test Accuracy | Macro F1 | Weighted F1 |\\n\"\n",
    "markdown_summary += \"|-------|---------------|----------|-------------|\\n\"\n",
    "\n",
    "for model_name in all_predictions.keys():\n",
    "    metrics = summary_report[model_name]\n",
    "    markdown_summary += f\"| {model_name} | {metrics['test_accuracy']:.4f} | {metrics['macro_f1']:.4f} | {metrics['weighted_f1']:.4f} |\\n\"\n",
    "\n",
    "markdown_summary += \"\\n## Generated Files\\n\\n\"\n",
    "markdown_summary += \"- **Classification Reports**: `classification_report_*.txt`\\n\"\n",
    "markdown_summary += \"- **Per-Class Metrics**: `per_class_metrics_*.csv`\\n\"\n",
    "markdown_summary += \"- **Confusion Matrices**: `confusion_matrix_*_detailed.png`\\n\"\n",
    "markdown_summary += \"- **Overall Comparison**: `confusion_matrices_all_models.png`\\n\"\n",
    "markdown_summary += \"- **Metrics Visualization**: `per_class_metrics_comparison.png`\\n\"\n",
    "markdown_summary += \"- **Comprehensive Report**: `model_comparison_comprehensive_*.json`\\n\"\n",
    "\n",
    "md_file = f'model_comparison_summary_{timestamp}.md'\n",
    "with open(md_file, 'w') as f:\n",
    "    f.write(markdown_summary)\n",
    "print(f\"âœ“ Summary report saved to: {md_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ANALYSIS COMPLETE\".center(100))\n",
    "print(\"=\"*100)\n",
    "print(\"\\nGenerated outputs:\")\n",
    "print(f\"  - Classification reports (text)\")\n",
    "print(f\"  - Per-class metrics (CSV)\")\n",
    "print(f\"  - Confusion matrices (detailed PNG)\")\n",
    "print(f\"  - Comparison visualizations (PNG)\")\n",
    "print(f\"  - Comprehensive JSON report\")\n",
    "print(f\"  - Markdown summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3d6d4",
   "metadata": {},
   "source": [
    "## 6. Saliency Maps - Gradient-based Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saliency Map Generation Function\n",
    "def compute_saliency_maps(model, X_test, predictions, num_samples=5, model_type='default'):\n",
    "    \"\"\"\n",
    "    Compute saliency maps using gradient-based method\n",
    "    Shows which pixels have the highest gradient with respect to class prediction\n",
    "    \"\"\"\n",
    "    saliency_maps = []\n",
    "    sample_indices = []\n",
    "    predicted_classes = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for idx in range(min(num_samples, len(X_test))):\n",
    "        # Get input and set requires_grad\n",
    "        if model_type == 'flat':\n",
    "            X_input = torch.FloatTensor(X_test[idx].reshape(1, -1)).to(device)\n",
    "        else:\n",
    "            X_input = torch.FloatTensor(X_test[idx:idx+1]).to(device)\n",
    "        \n",
    "        X_input.requires_grad = True\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(X_input)\n",
    "        pred_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Get the score for the predicted class\n",
    "        score = output[0, pred_class]\n",
    "        \n",
    "        # Backward pass to compute gradients\n",
    "        model.zero_grad()\n",
    "        score.backward()\n",
    "        \n",
    "        # Get saliency map (absolute value of gradients)\n",
    "        saliency = X_input.grad.data.abs()\n",
    "        \n",
    "        # Handle different input formats\n",
    "        if model_type == 'flat':\n",
    "            saliency = saliency.reshape(52, 52)\n",
    "            X_display = X_test[idx].reshape(52, 52)\n",
    "        elif model_type == '3ch':\n",
    "            # For 3-channel, take max across channels\n",
    "            saliency = saliency.max(dim=1)[0]\n",
    "            X_display = X_test[idx, 0]  # Display first channel (they're all the same)\n",
    "        else:  # 2d\n",
    "            X_display = X_test[idx].squeeze()\n",
    "            saliency = saliency.squeeze()\n",
    "        \n",
    "        saliency_maps.append(saliency.cpu().numpy())\n",
    "        sample_indices.append(idx)\n",
    "        predicted_classes.append(pred_class)\n",
    "    \n",
    "    return saliency_maps, sample_indices, predicted_classes\n",
    "\n",
    "print(\"Computing saliency maps for all models...\\n\")\n",
    "all_saliency_maps = {}\n",
    "\n",
    "for model_name, model in loaded_models.items():\n",
    "    print(f\"  {model_name}...\")\n",
    "    \n",
    "    X_test, y_test_actual, model_type = test_data_map[model_name]\n",
    "    preds = all_predictions[model_name]\n",
    "    \n",
    "    saliency_maps, sample_idx, pred_classes = compute_saliency_maps(\n",
    "        model, X_test, preds, num_samples=5, model_type=model_type\n",
    "    )\n",
    "    \n",
    "    all_saliency_maps[model_name] = {\n",
    "        'saliency_maps': saliency_maps,\n",
    "        'sample_indices': sample_idx,\n",
    "        'predicted_classes': pred_classes,\n",
    "        'X_test': X_test,\n",
    "        'model_type': model_type\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ“ Saliency maps computed for all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8cc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize saliency maps for each model\n",
    "print(\"=\"*100)\n",
    "print(\"SALIENCY MAPS VISUALIZATION\".center(100))\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "for model_name, saliency_data in all_saliency_maps.items():\n",
    "    saliency_maps = saliency_data['saliency_maps']\n",
    "    predicted_classes = saliency_data['predicted_classes']\n",
    "    X_test = saliency_data['X_test']\n",
    "    model_type = saliency_data['model_type']\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Create figure with subplots: original image | saliency map | overlay for each sample\n",
    "    fig, axes = plt.subplots(len(saliency_maps), 3, figsize=(15, 5*len(saliency_maps)))\n",
    "    if len(saliency_maps) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle(f'{model_name} - Saliency Maps\\n(Showing gradient magnitude at each pixel)', \n",
    "                 fontsize=14, fontweight='bold', y=0.995)\n",
    "    \n",
    "    for sample_idx, (saliency_map, pred_class) in enumerate(zip(saliency_maps, predicted_classes)):\n",
    "        # Get original image\n",
    "        if model_type == '3ch':\n",
    "            original_img = X_test[sample_idx, 0]  # First channel (all same for replicated grayscale)\n",
    "        else:\n",
    "            original_img = X_test[sample_idx].squeeze() if model_type == '2d' else X_test[sample_idx].reshape(52, 52)\n",
    "        \n",
    "        # Normalize saliency map for visualization\n",
    "        saliency_normalized = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min() + 1e-8)\n",
    "        \n",
    "        # Plot original image\n",
    "        ax = axes[sample_idx, 0]\n",
    "        im0 = ax.imshow(original_img, cmap='gray')\n",
    "        ax.set_title(f'Original (Pred: Class {pred_class:02d})', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        plt.colorbar(im0, ax=ax, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Plot saliency map\n",
    "        ax = axes[sample_idx, 1]\n",
    "        im1 = ax.imshow(saliency_normalized, cmap='hot')\n",
    "        ax.set_title(f'Saliency Map\\n(Gradient Magnitude)', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        plt.colorbar(im1, ax=ax, fraction=0.046, pad=0.04, label='Gradient')\n",
    "        \n",
    "        # Plot overlay (original with saliency overlay)\n",
    "        ax = axes[sample_idx, 2]\n",
    "        # Create RGB image for overlay\n",
    "        overlay = np.stack([original_img, original_img, original_img], axis=-1)\n",
    "        # Create red heatmap for high gradients\n",
    "        heatmap = np.zeros_like(overlay)\n",
    "        heatmap[:, :, 0] = saliency_normalized  # Red channel = saliency\n",
    "        # Blend\n",
    "        alpha = 0.6\n",
    "        blended = (1 - alpha) * overlay + alpha * heatmap\n",
    "        im2 = ax.imshow(blended)\n",
    "        ax.set_title(f'Overlay (Saliency on Image)', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        plt.colorbar(im2, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'saliency_maps_{model_name.lower().replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  âœ“ Saliency maps saved to: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative saliency analysis - compare models on same samples\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPARATIVE SALIENCY ANALYSIS - Same Samples Across Models\".center(100))\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "# Select first common sample to analyze across all models\n",
    "num_comparison_samples = 3\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5*num_comparison_samples))\n",
    "gs = fig.add_gridspec(num_comparison_samples, 5, hspace=0.3, wspace=0.3)\n",
    "\n",
    "for sample_num in range(num_comparison_samples):\n",
    "    # Get original image from first model\n",
    "    first_model_name = list(all_saliency_maps.keys())[0]\n",
    "    X_test_first = all_saliency_maps[first_model_name]['X_test']\n",
    "    model_type_first = all_saliency_maps[first_model_name]['model_type']\n",
    "    \n",
    "    if model_type_first == '3ch':\n",
    "        original_img = X_test_first[sample_num, 0]\n",
    "    else:\n",
    "        original_img = X_test_first[sample_num].squeeze() if model_type_first == '2d' else X_test_first[sample_num].reshape(52, 52)\n",
    "    \n",
    "    # Plot original image\n",
    "    ax = fig.add_subplot(gs[sample_num, 0])\n",
    "    im = ax.imshow(original_img, cmap='gray')\n",
    "    ax.set_title(f'Sample {sample_num+1}\\n(Original)', fontweight='bold', fontsize=11)\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Plot saliency maps from all models\n",
    "    for model_idx, (model_name, saliency_data) in enumerate(all_saliency_maps.items()):\n",
    "        ax = fig.add_subplot(gs[sample_num, model_idx + 1])\n",
    "        \n",
    "        if sample_num < len(saliency_data['saliency_maps']):\n",
    "            saliency_map = saliency_data['saliency_maps'][sample_num]\n",
    "            pred_class = saliency_data['predicted_classes'][sample_num]\n",
    "            \n",
    "            saliency_normalized = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min() + 1e-8)\n",
    "            im = ax.imshow(saliency_normalized, cmap='hot')\n",
    "            ax.set_title(f'{model_name}\\n(Class {pred_class:02d})', fontweight='bold', fontsize=11)\n",
    "            ax.axis('off')\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label='Grad')\n",
    "\n",
    "fig.suptitle('Comparative Saliency Maps Across All Models\\n(Gradient Magnitude Visualization)', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.savefig('comparative_saliency_maps.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Comparative saliency maps saved to: comparative_saliency_maps.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print statistics about saliency patterns\n",
    "print(\"\\nSaliency Statistics by Model:\\n\")\n",
    "for model_name, saliency_data in all_saliency_maps.items():\n",
    "    saliency_maps = saliency_data['saliency_maps']\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    for i, saliency_map in enumerate(saliency_maps):\n",
    "        mean_grad = saliency_map.mean()\n",
    "        max_grad = saliency_map.max()\n",
    "        std_grad = saliency_map.std()\n",
    "        print(f\"  Sample {i+1}: Mean gradient={mean_grad:.4f}, Max={max_grad:.4f}, Std={std_grad:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb826a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial attention regions analysis\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SPATIAL ATTENTION REGIONS - Hotspot Analysis\".center(100))\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "def identify_attention_regions(saliency_map, threshold=0.75):\n",
    "    \"\"\"Identify high-gradient regions (attention hotspots)\"\"\"\n",
    "    max_val = saliency_map.max()\n",
    "    threshold_val = threshold * max_val\n",
    "    hotspot_mask = saliency_map > threshold_val\n",
    "    return hotspot_mask\n",
    "\n",
    "# Create heatmap showing attention regions for each model\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "fig.suptitle('Spatial Attention Regions by Model\\n(Red = High Gradient Regions, Threshold = 75% of max)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for model_idx, (model_name, saliency_data) in enumerate(all_saliency_maps.items()):\n",
    "    ax = axes[model_idx]\n",
    "    \n",
    "    # Average saliency maps across samples\n",
    "    avg_saliency = np.mean(saliency_data['saliency_maps'], axis=0)\n",
    "    \n",
    "    # Normalize\n",
    "    avg_saliency_norm = (avg_saliency - avg_saliency.min()) / (avg_saliency.max() - avg_saliency.min() + 1e-8)\n",
    "    \n",
    "    # Identify hotspots\n",
    "    hotspot_mask = identify_attention_regions(avg_saliency_norm, threshold=0.75)\n",
    "    \n",
    "    # Create visualization\n",
    "    display_img = np.zeros((52, 52, 3))\n",
    "    display_img[:, :, 0] = avg_saliency_norm  # Red channel for intensity\n",
    "    display_img[hotspot_mask] = [1, 0, 0]  # Red for high-gradient regions\n",
    "    \n",
    "    im = ax.imshow(display_img)\n",
    "    ax.set_title(f'{model_name}\\n(Red hotspots: high gradient)', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('spatial_attention_regions.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Spatial attention regions plot saved to: spatial_attention_regions.png\")\n",
    "plt.show()\n",
    "\n",
    "# Quantitative analysis of attention regions\n",
    "print(\"\\nAttention Region Analysis:\\n\")\n",
    "for model_name, saliency_data in all_saliency_maps.items():\n",
    "    avg_saliency = np.mean(saliency_data['saliency_maps'], axis=0)\n",
    "    avg_saliency_norm = (avg_saliency - avg_saliency.min()) / (avg_saliency.max() - avg_saliency.min() + 1e-8)\n",
    "    \n",
    "    hotspot_mask = identify_attention_regions(avg_saliency_norm, threshold=0.75)\n",
    "    hotspot_percentage = (hotspot_mask.sum() / hotspot_mask.size) * 100\n",
    "    \n",
    "    # Find center of attention\n",
    "    y_coords, x_coords = np.where(hotspot_mask)\n",
    "    if len(y_coords) > 0:\n",
    "        center_y = int(y_coords.mean())\n",
    "        center_x = int(x_coords.mean())\n",
    "    else:\n",
    "        center_y, center_x = 26, 26\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  High-gradient region percentage: {hotspot_percentage:.2f}%\")\n",
    "    print(f\"  Attention center: ({center_x}, {center_y})\")\n",
    "    print(f\"  Average gradient magnitude: {avg_saliency.mean():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save saliency analysis results and interpretability report\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SAVING SALIENCY ANALYSIS RESULTS\".center(100))\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "# Create comprehensive saliency analysis report\n",
    "saliency_report = {\n",
    "    'timestamp': timestamp,\n",
    "    'method': 'Gradient-based Saliency Maps',\n",
    "    'description': 'Shows which pixels have highest gradient with respect to class prediction',\n",
    "    'models_analyzed': list(all_saliency_maps.keys()),\n",
    "}\n",
    "\n",
    "# Add statistics for each model\n",
    "for model_name, saliency_data in all_saliency_maps.items():\n",
    "    saliency_maps = saliency_data['saliency_maps']\n",
    "    \n",
    "    # Compute statistics\n",
    "    all_gradients = np.concatenate([m.flatten() for m in saliency_maps])\n",
    "    \n",
    "    model_stats = {\n",
    "        'num_samples_analyzed': len(saliency_maps),\n",
    "        'gradient_statistics': {\n",
    "            'mean': float(all_gradients.mean()),\n",
    "            'std': float(all_gradients.std()),\n",
    "            'min': float(all_gradients.min()),\n",
    "            'max': float(all_gradients.max()),\n",
    "            'median': float(np.median(all_gradients)),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add spatial attention analysis\n",
    "    avg_saliency = np.mean(saliency_maps, axis=0)\n",
    "    avg_saliency_norm = (avg_saliency - avg_saliency.min()) / (avg_saliency.max() - avg_saliency.min() + 1e-8)\n",
    "    hotspot_mask = identify_attention_regions(avg_saliency_norm, threshold=0.75)\n",
    "    \n",
    "    y_coords, x_coords = np.where(hotspot_mask)\n",
    "    if len(y_coords) > 0:\n",
    "        model_stats['attention_region'] = {\n",
    "            'hotspot_percentage': float((hotspot_mask.sum() / hotspot_mask.size) * 100),\n",
    "            'center_x': int(x_coords.mean()),\n",
    "            'center_y': int(y_coords.mean()),\n",
    "        }\n",
    "    \n",
    "    saliency_report[model_name] = model_stats\n",
    "\n",
    "# Save report as JSON\n",
    "report_file = f'saliency_analysis_report_{timestamp}.json'\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(saliency_report, f, indent=4, default=str)\n",
    "print(f\"âœ“ Saliency analysis report saved to: {report_file}\")\n",
    "\n",
    "# Create markdown documentation\n",
    "md_saliency = \"# Saliency Map Analysis Report\\n\\n\"\n",
    "md_saliency += f\"**Generated:** {timestamp}\\n\\n\"\n",
    "md_saliency += \"## Method\\n\\n\"\n",
    "md_saliency += \"**Gradient-based Saliency Maps**: Shows which pixels have the highest gradient magnitude with respect to the predicted class score.\\n\\n\"\n",
    "md_saliency += \"- **Principle**: Compute âˆ‚(score)/âˆ‚(pixels) to identify pixels that most influence the model's prediction\\n\"\n",
    "md_saliency += \"- **Interpretation**: Bright regions indicate pixels with high gradient (important for prediction)\\n\"\n",
    "md_saliency += \"- **Use Case**: Model interpretability and debugging\\n\\n\"\n",
    "\n",
    "md_saliency += \"## Results by Model\\n\\n\"\n",
    "\n",
    "for model_name, stats in saliency_report.items():\n",
    "    if isinstance(stats, dict) and 'gradient_statistics' in stats:\n",
    "        md_saliency += f\"### {model_name}\\n\\n\"\n",
    "        grad_stats = stats['gradient_statistics']\n",
    "        md_saliency += f\"**Gradient Statistics:**\\n\"\n",
    "        md_saliency += f\"- Mean: {grad_stats['mean']:.6f}\\n\"\n",
    "        md_saliency += f\"- Std Dev: {grad_stats['std']:.6f}\\n\"\n",
    "        md_saliency += f\"- Range: [{grad_stats['min']:.6f}, {grad_stats['max']:.6f}]\\n\"\n",
    "        md_saliency += f\"- Median: {grad_stats['median']:.6f}\\n\\n\"\n",
    "        \n",
    "        if 'attention_region' in stats:\n",
    "            attn = stats['attention_region']\n",
    "            md_saliency += f\"**Attention Region Analysis:**\\n\"\n",
    "            md_saliency += f\"- High-gradient region: {attn['hotspot_percentage']:.2f}% of image\\n\"\n",
    "            md_saliency += f\"- Attention center: ({attn['center_x']}, {attn['center_y']})\\n\\n\"\n",
    "\n",
    "md_saliency += \"## Generated Visualizations\\n\\n\"\n",
    "md_saliency += \"1. **saliency_maps_*.png** - Individual saliency maps for each model\\n\"\n",
    "md_saliency += \"   - Left: Original wafer image\\n\"\n",
    "md_saliency += \"   - Middle: Saliency map (gradient magnitude)\\n\"\n",
    "md_saliency += \"   - Right: Overlay (saliency on original image)\\n\\n\"\n",
    "md_saliency += \"2. **comparative_saliency_maps.png** - Side-by-side comparison across models\\n\"\n",
    "md_saliency += \"   - Shows how different models focus on different regions\\n\\n\"\n",
    "md_saliency += \"3. **spatial_attention_regions.png** - Identified hotspots\\n\"\n",
    "md_saliency += \"   - Red regions indicate high-gradient areas (75th percentile threshold)\\n\\n\"\n",
    "\n",
    "md_saliency += \"## Interpretation Guide\\n\\n\"\n",
    "md_saliency += \"- **Bright pixels**: High influence on prediction (model pays attention to these areas)\\n\"\n",
    "md_saliency += \"- **Dark pixels**: Low influence on prediction (model ignores these areas)\\n\"\n",
    "md_saliency += \"- **Concentrated regions**: Model uses localized features\\n\"\n",
    "md_saliency += \"- **Distributed patterns**: Model uses global features\\n\"\n",
    "\n",
    "md_file = f'saliency_analysis_summary_{timestamp}.md'\n",
    "with open(md_file, 'w') as f:\n",
    "    f.write(md_saliency)\n",
    "print(f\"âœ“ Saliency analysis summary saved to: {md_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SALIENCY MAP ANALYSIS COMPLETE\".center(100))\n",
    "print(\"=\"*100)\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"  - saliency_maps_*.png (per-model saliency visualizations)\")\n",
    "print(\"  - comparative_saliency_maps.png (cross-model comparison)\")\n",
    "print(\"  - spatial_attention_regions.png (hotspot analysis)\")\n",
    "print(f\"  - saliency_analysis_report_{timestamp}.json (detailed metrics)\")\n",
    "print(f\"  - saliency_analysis_summary_{timestamp}.md (documentation)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
