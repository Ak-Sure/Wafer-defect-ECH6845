{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19494c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "sys.path.insert(0, os.getcwd())\n",
    "from data_loading import WaferDataLoader\n",
    "from utility import (setup_model_and_loaders, hyperparameter_tuning, \n",
    "                     evaluate_model, train_model)\n",
    "from models import SimpleNN\n",
    "from config import SIMPLE_NN_TUNING_GRID\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✓ All libraries imported successfully!\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f391c6",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using WaferDataLoader\n",
    "print(\"Loading wafer defect dataset...\")\n",
    "loader = WaferDataLoader()\n",
    "\n",
    "print(f\"✓ Dataset loaded successfully!\")\n",
    "print(f\"X shape: {loader.X.shape}\")\n",
    "print(f\"y shape: {loader.y.shape}\")\n",
    "print(f\"Number of classes: {loader.num_classes}\")\n",
    "\n",
    "# Get the data\n",
    "X = loader.X.astype('float32')\n",
    "y = loader.y\n",
    "\n",
    "# Normalize data\n",
    "X_min, X_max = X.min(), X.max()\n",
    "X_normalized = (X - X_min) / (X_max - X_min) if X_max > X_min else X\n",
    "\n",
    "print(f\"\\n✓ Normalization:\")\n",
    "print(f\"  Original range: [{X_min}, {X_max}]\")\n",
    "print(f\"  Normalized range: [{X_normalized.min():.4f}, {X_normalized.max():.4f}]\")\n",
    "\n",
    "# Split into train/val/test (70% / 15% / 15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_normalized, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data split complete:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(y)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(y)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82519191",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tuning grid\n",
    "print(\"=\"*80)\n",
    "print(\"SIMPLE NEURAL NETWORK - HYPERPARAMETER TUNING GRID\".center(80))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTuning Grid:\")\n",
    "for param, values in SIMPLE_NN_TUNING_GRID.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in SIMPLE_NN_TUNING_GRID.values()])\n",
    "print(f\"\\nTotal combinations to evaluate: {total_combinations}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ae032",
   "metadata": {},
   "source": [
    "## 3. Run Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleNN Hyperparameter Tuning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING HYPERPARAMETER TUNING\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "simplenn_results = hyperparameter_tuning(\n",
    "    SimpleNN, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "    param_grid=SIMPLE_NN_TUNING_GRID,\n",
    "    input_size=2704,\n",
    "    num_classes=38,\n",
    "    device=str(device),\n",
    "    num_epochs=25,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TUNING COMPLETE\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract top 5\n",
    "simplenn_top5 = simplenn_results['summary_df'].head(5).copy()\n",
    "print(\"\\nTop 5 Configurations:\")\n",
    "print(simplenn_top5[['learning_rate', 'batch_size', 'num_epochs', 'optimizer', 'Val_Acc', 'Test_Acc']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa23412",
   "metadata": {},
   "source": [
    "## 4. Validation Loss Curves - Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba75122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain top 5 models to get training histories\n",
    "print(\"\\nRetraining top 5 models to generate loss curves...\")\n",
    "\n",
    "def retrain_with_history(model_class, X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                         params, input_size, num_classes, device_str):\n",
    "    \"\"\"Retrain a model with given params and return training history\"\"\"\n",
    "    try:\n",
    "        # Extract parameters\n",
    "        learning_rate = params.get('learning_rate', 0.001)\n",
    "        batch_size = params.get('batch_size', 64)\n",
    "        optimizer_type = params.get('optimizer', 'adam')\n",
    "        epochs_to_train = params.get('num_epochs', 20)\n",
    "        \n",
    "        # Setup model and loaders\n",
    "        setup_result = setup_model_and_loaders(\n",
    "            model_class, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "            input_size=input_size, num_classes=num_classes, device=device_str,\n",
    "            batch_size=batch_size, model_kwargs={}, verbose=False\n",
    "        )\n",
    "        \n",
    "        model = setup_result['model']\n",
    "        train_loader = setup_result['train_loader']\n",
    "        val_loader = setup_result['val_loader']\n",
    "        \n",
    "        # Setup optimizer\n",
    "        if optimizer_type.lower() == 'adam':\n",
    "            opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            opt = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        \n",
    "        # Train\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        history = train_model(\n",
    "            model, train_loader, val_loader, criterion, opt,\n",
    "            num_epochs=epochs_to_train, device=device_str, patience=5\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get histories for top 5\n",
    "top5_histories = []\n",
    "for i, (_, row) in enumerate(simplenn_top5.iterrows(), 1):\n",
    "    print(f\"  Retraining rank {i}/5...\")\n",
    "    \n",
    "    # Extract parameters from row (they are individual columns, not a 'params' dict)\n",
    "    params = {\n",
    "        'learning_rate': row['learning_rate'],\n",
    "        'batch_size': int(row['batch_size']),\n",
    "        'optimizer': row['optimizer'],\n",
    "        'num_epochs': int(row['num_epochs']),\n",
    "    }\n",
    "    \n",
    "    history = retrain_with_history(\n",
    "        SimpleNN, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        params, input_size=2704, num_classes=38, device_str=str(device)\n",
    "    )\n",
    "    if history:\n",
    "        top5_histories.append(history)\n",
    "\n",
    "print(f\"✓ Successfully generated {len(top5_histories)} training histories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f44abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation loss curves for top 5\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('Simple Neural Network - Top 5 Configurations\\nTraining vs Validation Loss', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, history in enumerate(top5_histories):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "    ax.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "    \n",
    "    rank = idx + 1\n",
    "    val_acc = simplenn_top5.iloc[idx]['Val_Acc']\n",
    "    ax.set_title(f'Rank {rank} - Val Acc: {val_acc:.4f}', fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[1, 2].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('simplenn_validation_loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Loss curves plot saved as 'simplenn_validation_loss_curves.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3ff67",
   "metadata": {},
   "source": [
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f96588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = 'simplenn_results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Save summary CSV with all results\n",
    "simplenn_results['summary_df'].to_csv(f'{results_dir}/all_results_{timestamp}.csv', index=False)\n",
    "print(f\"✓ All results saved to: all_results_{timestamp}.csv\")\n",
    "\n",
    "# 2. Save top 5 CSV\n",
    "simplenn_top5.to_csv(f'{results_dir}/top5_results_{timestamp}.csv', index=False)\n",
    "print(f\"✓ Top 5 results saved to: top5_results_{timestamp}.csv\")\n",
    "\n",
    "# 3. Save best model info\n",
    "best_model_info = {\n",
    "    'best_params': simplenn_results['best_params'],\n",
    "    'best_val_acc': float(simplenn_results['best_val_acc']),\n",
    "    'best_test_acc': float(simplenn_results['best_test_acc']),\n",
    "}\n",
    "\n",
    "with open(f'{results_dir}/best_model_{timestamp}.json', 'w') as f:\n",
    "    json.dump(best_model_info, f, indent=4, default=str)\n",
    "print(f\"✓ Best model info saved to: best_model_{timestamp}.json\")\n",
    "\n",
    "# 4. Save best model itself\n",
    "torch.save(simplenn_results['best_model'].state_dict(), \n",
    "           f'{results_dir}/best_model_weights_{timestamp}.pt')\n",
    "print(f\"✓ Best model weights saved to: best_model_weights_{timestamp}.pt\")\n",
    "\n",
    "# 5. Save training history for best model\n",
    "with open(f'{results_dir}/best_model_history_{timestamp}.pkl', 'wb') as f:\n",
    "    pickle.dump(simplenn_results['best_history'], f)\n",
    "print(f\"✓ Best model training history saved to: best_model_history_{timestamp}.pkl\")\n",
    "\n",
    "# 6. Save training histories for top 5\n",
    "for i, history in enumerate(top5_histories, 1):\n",
    "    with open(f'{results_dir}/rank_{i:02d}_history_{timestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "print(f\"✓ Top 5 training histories saved\")\n",
    "\n",
    "# 7. Save loss curves data as CSV for easy access\n",
    "for i, history in enumerate(top5_histories, 1):\n",
    "    loss_df = pd.DataFrame({\n",
    "        'Epoch': range(1, len(history['train_loss']) + 1),\n",
    "        'Train_Loss': history['train_loss'],\n",
    "        'Val_Loss': history['val_loss'],\n",
    "        'Train_Acc': history['train_acc'],\n",
    "        'Val_Acc': history['val_acc'],\n",
    "    })\n",
    "    loss_df.to_csv(f'{results_dir}/rank_{i:02d}_loss_curves_{timestamp}.csv', index=False)\n",
    "print(f\"✓ Loss curves data saved as CSV for all top 5\")\n",
    "\n",
    "# 8. Save summary report\n",
    "summary = {\n",
    "    'model': 'SimpleNN',\n",
    "    'timestamp': timestamp,\n",
    "    'total_combinations': int(total_combinations),\n",
    "    'best_val_accuracy': float(simplenn_results['best_val_acc']),\n",
    "    'best_test_accuracy': float(simplenn_results['best_test_acc']),\n",
    "    'best_hyperparameters': simplenn_results['best_params'],\n",
    "    'top5_accuracies': simplenn_top5['Val_Acc'].tolist(),\n",
    "}\n",
    "\n",
    "with open(f'{results_dir}/summary_report_{timestamp}.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4, default=str)\n",
    "print(f\"✓ Summary report saved to: summary_report_{timestamp}.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"All results saved to: {results_dir}/\".center(80))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa024c4",
   "metadata": {},
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIMPLE NEURAL NETWORK - TUNING RESULTS SUMMARY\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nBest Model Performance:\")\n",
    "print(f\"  Validation Accuracy: {simplenn_results['best_val_acc']:.4f}\")\n",
    "print(f\"  Test Accuracy: {simplenn_results['best_test_acc']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for key, value in simplenn_results['best_params'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nTop 5 Validation Accuracies:\")\n",
    "for rank, acc in enumerate(simplenn_top5['Val_Acc'].values, 1):\n",
    "    print(f\"  Rank {rank}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal tuning combinations evaluated: {total_combinations}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4ac49",
   "metadata": {},
   "source": [
    "## 6. Best Model Evaluation - Classification Report & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and generate predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "print(\"Loading best model and generating predictions...\\n\")\n",
    "\n",
    "best_model = simplenn_results['best_model']\n",
    "best_model.eval()\n",
    "\n",
    "# Generate predictions on test set\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    predictions = predictions.cpu().numpy()\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"✓ Best Model Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "\n",
    "# Generate classification report\n",
    "class_names = [f'Class_{i:02d}' for i in range(38)]\n",
    "print(\"=\"*100)\n",
    "print(\"CLASSIFICATION REPORT - Best SimpleNN Model\".center(100))\n",
    "print(\"=\"*100)\n",
    "report = classification_report(y_test, predictions, target_names=class_names, digits=4)\n",
    "print(report)\n",
    "\n",
    "with open(f'{results_dir}/classification_report_{timestamp}.txt', 'w') as f:\n",
    "    f.write(f\"Classification Report - Best SimpleNN Model\\n\")\n",
    "    f.write(f\"{'='*100}\\n\\n\")\n",
    "    f.write(report)\n",
    "print(f\"✓ Classification report saved\\n\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(f\"✓ Confusion matrix computed: shape {cm.shape}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax,\n",
    "            xticklabels=range(38), yticklabels=range(38), cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Best SimpleNN Model\\n(Test Accuracy: {:.4f})'.format(test_accuracy),\n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Class', fontsize=14)\n",
    "plt.ylabel('True Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/confusion_matrix_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Confusion matrix plot saved\\n\")\n",
    "plt.show()\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, predictions, average=None)\n",
    "class_accuracy = recall\n",
    "\n",
    "class_metrics = pd.DataFrame({\n",
    "    'Class': [f'Class_{i:02d}' for i in range(38)],\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support,\n",
    "    'Accuracy': class_accuracy\n",
    "})\n",
    "\n",
    "class_metrics.to_csv(f'{results_dir}/class_wise_metrics_{timestamp}.csv', index=False)\n",
    "print(\"CLASS-WISE PERFORMANCE METRICS:\")\n",
    "print(\"=\"*100)\n",
    "print(class_metrics.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "colors = ['green' if acc >= 0.8 else 'orange' if acc >= 0.6 else 'red' for acc in class_accuracy]\n",
    "ax.bar(range(38), class_accuracy, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.axhline(y=class_accuracy.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean: {class_accuracy.mean():.4f}')\n",
    "ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (Recall)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class Accuracy', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(range(0, 38, 4))\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(recall, precision, s=100, alpha=0.6, c=range(38), cmap='viridis', edgecolors='black')\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Precision vs Recall', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "colors_f1 = ['green' if f >= 0.8 else 'orange' if f >= 0.6 else 'red' for f in f1]\n",
    "ax.bar(range(38), f1, color=colors_f1, alpha=0.7, edgecolor='black')\n",
    "ax.axhline(y=f1.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean: {f1.mean():.4f}')\n",
    "ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class F1-Score', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(range(0, 38, 4))\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.bar(range(38), support, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Sample Count', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Test Set Distribution', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(range(0, 38, 4))\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('SimpleNN Best Model - Class-Wise Performance', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/class_wise_accuracy_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Class-wise accuracy plots saved\\n\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Overall Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Macro F1-Score: {f1.mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
